{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kavita5/twitter-dataset-avengersendgame/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "Create Target column from retweetCount > np.median[retweetCount] \n",
    "Create dataset in .csv file with new features along with .yaml file with it's descritpion, median[retweetCount] value stored in retweetCount_median variable, preferably store data in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NRmalaa</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>jijitsuu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "1           2  RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...      False   \n",
       "2           3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "1              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "2              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  \\\n",
       "0  1120639328034676737         NaN   \n",
       "1  1120639325199196160         NaN   \n",
       "2  1120639324683292674         NaN   \n",
       "\n",
       "                                        statusSource screenName  retweetCount  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  DavidAc96           637   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...    NRmalaa           302   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   jijitsuu             0   \n",
       "\n",
       "   isRetweet  retweeted  longitude  latitude  \n",
       "0       True      False        NaN       NaN  \n",
       "1       True      False        NaN       NaN  \n",
       "2      False      False        NaN       NaN  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('tweets.csv', encoding='cp1252')\n",
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3         True\n",
       "4         True\n",
       "         ...  \n",
       "14995    False\n",
       "14996     True\n",
       "14997    False\n",
       "14998     True\n",
       "14999     True\n",
       "Name: retweetCount, Length: 15000, dtype: bool"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['retweetCount'] > np.median(tweets_df['retweetCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NRmalaa</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>jijitsuu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "1           2  RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...      False   \n",
       "2           3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "1              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "2              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  \\\n",
       "0  1120639328034676737         NaN   \n",
       "1  1120639325199196160         NaN   \n",
       "2  1120639324683292674         NaN   \n",
       "\n",
       "                                        statusSource screenName  retweetCount  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  DavidAc96           637   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...    NRmalaa           302   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   jijitsuu             0   \n",
       "\n",
       "   isRetweet  retweeted  longitude  latitude  High_Retweet_Count  \n",
       "0       True      False        NaN       NaN                   0  \n",
       "1       True      False        NaN       NaN                   0  \n",
       "2      False      False        NaN       NaN                   0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['High_Retweet_Count'] = 0\n",
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NRmalaa</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>jijitsuu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:29</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639323328540672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>SahapunB</td>\n",
       "      <td>23781</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:29</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639321571074048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>stella22_97</td>\n",
       "      <td>13067</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "1           2  RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...      False   \n",
       "2           3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
       "3           4  RT @HelloBoon: Man these #AvengersEndgame ads ...      False   \n",
       "4           5  RT @Marvel: We salute you, @ChrisEvans! #Capta...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "1              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "2              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "3              0       NaN  2019-04-23 10:43:29      False         NaN   \n",
       "4              0       NaN  2019-04-23 10:43:29      False         NaN   \n",
       "\n",
       "                    id  replyToUID  \\\n",
       "0  1120639328034676737         NaN   \n",
       "1  1120639325199196160         NaN   \n",
       "2  1120639324683292674         NaN   \n",
       "3  1120639323328540672         NaN   \n",
       "4  1120639321571074048         NaN   \n",
       "\n",
       "                                        statusSource   screenName  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...    DavidAc96   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...      NRmalaa   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...     jijitsuu   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...     SahapunB   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...  stella22_97   \n",
       "\n",
       "   retweetCount  isRetweet  retweeted  longitude  latitude  High_Retweet_Count  \n",
       "0           637       True      False        NaN       NaN               False  \n",
       "1           302       True      False        NaN       NaN               False  \n",
       "2             0      False      False        NaN       NaN               False  \n",
       "3         23781       True      False        NaN       NaN                True  \n",
       "4         13067       True      False        NaN       NaN                True  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['High_Retweet_Count'] = tweets_df['retweetCount'] > np.median(tweets_df['retweetCount'])\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "      <th>Bin_High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NRmalaa</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>jijitsuu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @HelloBoon: Man these #AvengersEndgame ads ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:29</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639323328540672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>SahapunB</td>\n",
       "      <td>23781</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:29</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639321571074048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>stella22_97</td>\n",
       "      <td>13067</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>14996</td>\n",
       "      <td>RT @natsdany: First time                  Last...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 09:22:03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120618828918951937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>tommysboi</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>14997</td>\n",
       "      <td>RT @MTVNEWS: The #AvengersEndgame cast has see...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 09:22:03</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120618828038311936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>kimberleywithae</td>\n",
       "      <td>2307</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>14998</td>\n",
       "      <td>@SPICinemas kindly announce the approximate ti...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>SPICinemas</td>\n",
       "      <td>2019-04-23 09:22:02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120618823667920896</td>\n",
       "      <td>919079586.0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Gnanavel07</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>14999</td>\n",
       "      <td>RT @Marvel: We salute you, @ChrisEvans! #Capta...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 09:22:02</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120618823600803840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>_moonljght</td>\n",
       "      <td>13167</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>15000</td>\n",
       "      <td>RT @Avengers: Welcome to the party, @RobertDow...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 09:22:01</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120618822300569601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>CaterinaCabrel1</td>\n",
       "      <td>10736</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  \\\n",
       "0               1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...   \n",
       "1               2  RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...   \n",
       "2               3  saving these bingo cards for tomorrow \\r\\n©\\r\\...   \n",
       "3               4  RT @HelloBoon: Man these #AvengersEndgame ads ...   \n",
       "4               5  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
       "...           ...                                                ...   \n",
       "14995       14996  RT @natsdany: First time                  Last...   \n",
       "14996       14997  RT @MTVNEWS: The #AvengersEndgame cast has see...   \n",
       "14997       14998  @SPICinemas kindly announce the approximate ti...   \n",
       "14998       14999  RT @Marvel: We salute you, @ChrisEvans! #Capta...   \n",
       "14999       15000  RT @Avengers: Welcome to the party, @RobertDow...   \n",
       "\n",
       "       favorited  favoriteCount   replyToSN              created  truncated  \\\n",
       "0          False              0         NaN  2019-04-23 10:43:30      False   \n",
       "1          False              0         NaN  2019-04-23 10:43:30      False   \n",
       "2          False              0         NaN  2019-04-23 10:43:30      False   \n",
       "3          False              0         NaN  2019-04-23 10:43:29      False   \n",
       "4          False              0         NaN  2019-04-23 10:43:29      False   \n",
       "...          ...            ...         ...                  ...        ...   \n",
       "14995      False              0         NaN  2019-04-23 09:22:03      False   \n",
       "14996      False              0         NaN  2019-04-23 09:22:03      False   \n",
       "14997      False              0  SPICinemas  2019-04-23 09:22:02      False   \n",
       "14998      False              0         NaN  2019-04-23 09:22:02      False   \n",
       "14999      False              0         NaN  2019-04-23 09:22:01      False   \n",
       "\n",
       "       replyToSID                   id   replyToUID  \\\n",
       "0             NaN  1120639328034676737          NaN   \n",
       "1             NaN  1120639325199196160          NaN   \n",
       "2             NaN  1120639324683292674          NaN   \n",
       "3             NaN  1120639323328540672          NaN   \n",
       "4             NaN  1120639321571074048          NaN   \n",
       "...           ...                  ...          ...   \n",
       "14995         NaN  1120618828918951937          NaN   \n",
       "14996         NaN  1120618828038311936          NaN   \n",
       "14997         NaN  1120618823667920896  919079586.0   \n",
       "14998         NaN  1120618823600803840          NaN   \n",
       "14999         NaN  1120618822300569601          NaN   \n",
       "\n",
       "                                            statusSource       screenName  \\\n",
       "0      <a href=\"http://twitter.com/download/android\" ...        DavidAc96   \n",
       "1      <a href=\"http://twitter.com/download/iphone\" r...          NRmalaa   \n",
       "2      <a href=\"http://twitter.com/download/iphone\" r...         jijitsuu   \n",
       "3      <a href=\"http://twitter.com/download/iphone\" r...         SahapunB   \n",
       "4      <a href=\"http://twitter.com/download/iphone\" r...      stella22_97   \n",
       "...                                                  ...              ...   \n",
       "14995  <a href=\"http://twitter.com/download/android\" ...        tommysboi   \n",
       "14996  <a href=\"http://twitter.com/download/iphone\" r...  kimberleywithae   \n",
       "14997  <a href=\"http://twitter.com/download/android\" ...       Gnanavel07   \n",
       "14998  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       _moonljght   \n",
       "14999  <a href=\"http://twitter.com/download/android\" ...  CaterinaCabrel1   \n",
       "\n",
       "       retweetCount  isRetweet  retweeted  longitude  latitude  \\\n",
       "0               637       True      False        NaN       NaN   \n",
       "1               302       True      False        NaN       NaN   \n",
       "2                 0      False      False        NaN       NaN   \n",
       "3             23781       True      False        NaN       NaN   \n",
       "4             13067       True      False        NaN       NaN   \n",
       "...             ...        ...        ...        ...       ...   \n",
       "14995            33       True      False        NaN       NaN   \n",
       "14996          2307       True      False        NaN       NaN   \n",
       "14997             0      False      False        NaN       NaN   \n",
       "14998         13167       True      False        NaN       NaN   \n",
       "14999         10736       True      False        NaN       NaN   \n",
       "\n",
       "       High_Retweet_Count  Bin_High_Retweet_Count  \n",
       "0                   False                       1  \n",
       "1                   False                       1  \n",
       "2                   False                       1  \n",
       "3                    True                       0  \n",
       "4                    True                       0  \n",
       "...                   ...                     ...  \n",
       "14995               False                       1  \n",
       "14996                True                       0  \n",
       "14997               False                       1  \n",
       "14998                True                       0  \n",
       "14999                True                       0  \n",
       "\n",
       "[15000 rows x 19 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['Bin_High_Retweet_Count'] = tweets_df['High_Retweet_Count'].apply(lambda x: 1 if x!= True  else 0)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('./data/new_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/tweets.yaml\n"
     ]
    }
   ],
   "source": [
    "in collaboration%%writefile './data/tweets.yaml'\n",
    "\n",
    "description: median[retweetCount] value stored in retweetCount_median variable\n",
    "    \n",
    "median_retweet_count: 1755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Collaborate at github account\n",
    "\n",
    "https://github.com/maxmmsu/harbour.space.DS.402\n",
    "\n",
    "Create an .md file in it with the table of participants (one for a group), select a short acronim for your name (e.g. Maxim Musin --> mm) and write down the acronim to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Сreate a folder with your first exeperimet, name it exp_[your name acronim]_1, make it a python package, organize all transformers and classifiers wih python modules and .yaml files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Logic\n",
    "# Build Logic\n",
    "# Build Logic\n",
    "# Build Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Logic 5\n",
    "\n",
    "## Select one of the following feature creation technics, write it down to collaboration table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Select one of the following feature creation technics, write it down to collaboration table\n",
    "\n",
    "##### Step 1 create 3 of the following features\n",
    "- Extract lenght of the sentence\n",
    "- Extract number of words\n",
    "- Extract number of characters\n",
    "- Extract number of hastags\n",
    "\n",
    "- Create a dummy variable for RT at the beginning of the twit\n",
    "- Creatu dummy variable from first letter of twit being A, B, C or D\n",
    "\n",
    "##### Step 2 preform polynomial feature generation with parameter k = 2..6 \n",
    "\n",
    "##### Step 3 perform dimensionality reduction to limit number of vatiables\n",
    "- perform PCA dimensionality reduction\n",
    "- *prefromr PCA dimensionality reduction according to covered variance\n",
    "- select features with maximum mutual information with target variable\n",
    "- *select features according to sum of adjusted mutual info score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "#### Extract number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/berend/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_words(sentence):\n",
    "    sentence_array = word_tokenize(sentence)\n",
    "    words = [word for word in sentence_array if word.isalpha()]\n",
    "    return len(words)\n",
    "\n",
    "tweets_df['number_of_words'] = tweets_df['text'].apply(number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_chars(sentence):\n",
    "    return len(sentence)\n",
    "\n",
    "tweets_df['number_of_chars'] = tweets_df['text'].apply(number_of_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract number of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_hashtags(sentence):\n",
    "    return len(re.findall('#\\w+', sentence))\n",
    "\n",
    "tweets_df['number_of_hashtags'] = tweets_df['text'].apply(number_of_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "#### Polynomial features, degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>number_of_chars</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>Bin_High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>108</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>102</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       favoriteCount  number_of_words  number_of_chars  number_of_hashtags  \\\n",
       "0                  0                7               81                   1   \n",
       "1                  0               10              108                   2   \n",
       "2                  0                8               84                   1   \n",
       "3                  0                9               84                   1   \n",
       "4                  0                9               96                   2   \n",
       "...              ...              ...              ...                 ...   \n",
       "14995              0               12              144                   0   \n",
       "14996              0               26              140                   1   \n",
       "14997              0               16              140                   2   \n",
       "14998              0                9               96                   2   \n",
       "14999              0               10              102                   2   \n",
       "\n",
       "       Bin_High_Retweet_Count  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           1  \n",
       "3                           0  \n",
       "4                           0  \n",
       "...                       ...  \n",
       "14995                       1  \n",
       "14996                       0  \n",
       "14997                       1  \n",
       "14998                       0  \n",
       "14999                       0  \n",
       "\n",
       "[15000 rows x 5 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features = ['favoriteCount', 'number_of_words', 'number_of_chars', 'number_of_hashtags','Bin_High_Retweet_Count']\n",
    "\n",
    "tweets_df_num =  tweets_df[numerical_features]\n",
    "\n",
    "tweets_df_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 56)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = preprocessing.PolynomialFeatures(degree=3)\n",
    "\n",
    "transformed_df = poly.fit_transform(tweets_df[numerical_features])\n",
    "\n",
    "transformed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "#### Perform PCA reduction according to covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 56)\n",
      "(11250, 1)\n"
     ]
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=0.90, svd_solver='full')\n",
    "pca.fit(poly_data_train)\n",
    "\n",
    "pca_data_train = pca.transform(scaled_data_train)\n",
    "pca_data_test = pca.transform(scaled_data_test)\n",
    "\n",
    "pca_data_df = DataFrame (pca_data_train)\n",
    "\n",
    "print(transformed_df.shape)\n",
    "print(pca_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select features with maximum mutual information with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.071307</td>\n",
       "      <td>-0.503257</td>\n",
       "      <td>-0.350043</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603432</td>\n",
       "      <td>-0.620538</td>\n",
       "      <td>-0.601080</td>\n",
       "      <td>-0.603939</td>\n",
       "      <td>-0.576943</td>\n",
       "      <td>-0.399602</td>\n",
       "      <td>-0.478251</td>\n",
       "      <td>-0.496683</td>\n",
       "      <td>-0.363521</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.639559</td>\n",
       "      <td>-0.692243</td>\n",
       "      <td>-1.062257</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.685249</td>\n",
       "      <td>-0.790731</td>\n",
       "      <td>-0.678828</td>\n",
       "      <td>-0.917209</td>\n",
       "      <td>-0.718380</td>\n",
       "      <td>-0.414390</td>\n",
       "      <td>-0.912638</td>\n",
       "      <td>-0.681848</td>\n",
       "      <td>-0.390683</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.705724</td>\n",
       "      <td>0.819644</td>\n",
       "      <td>1.036900</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509130</td>\n",
       "      <td>0.762876</td>\n",
       "      <td>0.150487</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>0.096905</td>\n",
       "      <td>-0.296083</td>\n",
       "      <td>0.928978</td>\n",
       "      <td>-0.032547</td>\n",
       "      <td>-0.310626</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  0.0 -0.055742 -0.071307 -0.503257 -0.350043 -0.537388 -0.025841 -0.016156   \n",
       "1  0.0 -0.055742 -0.639559 -0.692243 -1.062257 -0.537388 -0.025841 -0.016156   \n",
       "2  0.0 -0.055742 -0.705724  0.819644  1.036900 -0.537388 -0.025841 -0.016156   \n",
       "\n",
       "         8         9   ...        46        47        48        49        50  \\\n",
       "0 -0.046751 -0.051346  ... -0.603432 -0.620538 -0.601080 -0.603939 -0.576943   \n",
       "1 -0.046751 -0.051346  ... -0.685249 -0.790731 -0.678828 -0.917209 -0.718380   \n",
       "2 -0.046751 -0.051346  ...  0.509130  0.762876  0.150487  0.993506  0.096905   \n",
       "\n",
       "         51        52        53        54        55  \n",
       "0 -0.399602 -0.478251 -0.496683 -0.363521 -0.214865  \n",
       "1 -0.414390 -0.912638 -0.681848 -0.390683 -0.214865  \n",
       "2 -0.296083  0.928978 -0.032547 -0.310626 -0.214865  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=56, step=1)\n"
     ]
    }
   ],
   "source": [
    "MI = sklearn.feature_selection.mutual_info_classif(scaled_train_df, y_train)\n",
    "MI = pd.Series(MI)\n",
    "MI.index = scaled_train_df.columns\n",
    "print(MI.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MI.sort_values(ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "MI.plot.bar(figsize = (16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "sel = SelectPercentile(mutual_info_classif, percentile = 25).fit(scaled_data_train, y_train)\n",
    "indices_selection = scaled_train_df.columns[sel.get_support()]\n",
    "\n",
    "indices_selection\n",
    "print(len(indices_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 14)\n",
      "(3750, 14)\n"
     ]
    }
   ],
   "source": [
    "# New dataframe based on best MI scores!\n",
    "\n",
    "X_train_MI = sel.transform(scaled_train_df)\n",
    "X_test_MI = sel.transform(scaled_test_df)\n",
    "\n",
    "print(X_train_MI.shape)\n",
    "print(X_test_MI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select features according to sum of adjusted mutual info score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.071307</td>\n",
       "      <td>-0.503257</td>\n",
       "      <td>-0.350043</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603432</td>\n",
       "      <td>-0.620538</td>\n",
       "      <td>-0.601080</td>\n",
       "      <td>-0.603939</td>\n",
       "      <td>-0.576943</td>\n",
       "      <td>-0.399602</td>\n",
       "      <td>-0.478251</td>\n",
       "      <td>-0.496683</td>\n",
       "      <td>-0.363521</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.639559</td>\n",
       "      <td>-0.692243</td>\n",
       "      <td>-1.062257</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.685249</td>\n",
       "      <td>-0.790731</td>\n",
       "      <td>-0.678828</td>\n",
       "      <td>-0.917209</td>\n",
       "      <td>-0.718380</td>\n",
       "      <td>-0.414390</td>\n",
       "      <td>-0.912638</td>\n",
       "      <td>-0.681848</td>\n",
       "      <td>-0.390683</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.055742</td>\n",
       "      <td>-0.705724</td>\n",
       "      <td>0.819644</td>\n",
       "      <td>1.036900</td>\n",
       "      <td>-0.537388</td>\n",
       "      <td>-0.025841</td>\n",
       "      <td>-0.016156</td>\n",
       "      <td>-0.046751</td>\n",
       "      <td>-0.051346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509130</td>\n",
       "      <td>0.762876</td>\n",
       "      <td>0.150487</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>0.096905</td>\n",
       "      <td>-0.296083</td>\n",
       "      <td>0.928978</td>\n",
       "      <td>-0.032547</td>\n",
       "      <td>-0.310626</td>\n",
       "      <td>-0.214865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  0.0 -0.055742 -0.071307 -0.503257 -0.350043 -0.537388 -0.025841 -0.016156   \n",
       "1  0.0 -0.055742 -0.639559 -0.692243 -1.062257 -0.537388 -0.025841 -0.016156   \n",
       "2  0.0 -0.055742 -0.705724  0.819644  1.036900 -0.537388 -0.025841 -0.016156   \n",
       "\n",
       "         8         9   ...        46        47        48        49        50  \\\n",
       "0 -0.046751 -0.051346  ... -0.603432 -0.620538 -0.601080 -0.603939 -0.576943   \n",
       "1 -0.046751 -0.051346  ... -0.685249 -0.790731 -0.678828 -0.917209 -0.718380   \n",
       "2 -0.046751 -0.051346  ...  0.509130  0.762876  0.150487  0.993506  0.096905   \n",
       "\n",
       "         51        52        53        54        55  \n",
       "0 -0.399602 -0.478251 -0.496683 -0.363521 -0.214865  \n",
       "1 -0.414390 -0.912638 -0.681848 -0.390683 -0.214865  \n",
       "2 -0.296083  0.928978 -0.032547 -0.310626 -0.214865  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AMI_list = []\n",
    "for i in range(len(scaled_train_df.columns)):\n",
    "    AMI = sklearn.metrics.adjusted_mutual_info_score(scaled_train_df[i],y_train)\n",
    "    AMI_list.append(AMI)\n",
    "AMI = np.array(AMI_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=56, step=1)\n"
     ]
    }
   ],
   "source": [
    "AMI = pd.Series(AMI)\n",
    "AMI.index = scaled_train_df.columns\n",
    "print(AMI.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AMI.sort_values(ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "AMI.plot.bar(figsize = (16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "indices_selection2 = AMI[AMI > 0.0005].index\n",
    "print(len(indices_selection2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11250, 55)\n",
      "(3750, 55)\n"
     ]
    }
   ],
   "source": [
    "X_train_AMI = scaled_train_df[indices_selection2]\n",
    "X_test_AMI = scaled_test_df[indices_selection2]\n",
    "\n",
    "print(X_train_AMI.shape)\n",
    "print(X_test_AMI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build logic 6\n",
    "\n",
    "## Extract names of avengers endgame characters from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: Extract names of avengers endgame characters from text \n",
    "\n",
    "Examples of tags in question:\n",
    "#Thanos \n",
    "#WinterSoldier \n",
    "#CaptainAmerica \n",
    "#blackwidow\n",
    "#CaptainMarvel\n",
    "#Mantis \n",
    "\n",
    "store the resulting dataset in data folder with .yaml description, preferable naming for variable names is lowercase name of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/berend/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package stopwords to /home/berend/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('english') + stopwords_verbs + stopwords_other\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wintersoldier': [],\n",
       " 'blackwidow': [],\n",
       " 'captainmarvel': [],\n",
       " 'manntis': [],\n",
       " 'ebony maw': ['maw', \"ebony maw's\"],\n",
       " 'thanos': [],\n",
       " 'thor': [],\n",
       " 'captainamerica': [],\n",
       " 'loki': [],\n",
       " 'heimdall': [],\n",
       " 'proxima midnight': [],\n",
       " 'stephen strange': ['strange',\n",
       "  'dr strange',\n",
       "  'doctor strange',\n",
       "  \"doctor strange's\",\n",
       "  'doc'],\n",
       " 'wong': [],\n",
       " 'bruce banner': ['hulk', 'banner', 'bruce'],\n",
       " 'tony stark': [\"iron man's\", 'iron man', 'stark', 'tony'],\n",
       " 'pepper potts': ['pepper', 'potts'],\n",
       " 'f.r.i.d.a.y.': [],\n",
       " 'peter parker': ['spiderman', 'iron spider', 'peter'],\n",
       " 'ned leeds': [],\n",
       " 'school bus driver, stan lee': [],\n",
       " 'cull obsidian': [\"cull obsidian's\"],\n",
       " 'peter quill': ['quill', 'star lord'],\n",
       " 'rocket': [],\n",
       " 'gamora': ['young gamora', 'memory gamora'],\n",
       " 'drax': [],\n",
       " 'mantis': [],\n",
       " 'groot': [],\n",
       " 'vision': [],\n",
       " 'wanda maximoff': ['wanda maximoff', 'wanda', 'scarlet'],\n",
       " 'corvus glaive': ['corvus', \"corvus glaive's\", 'glaive'],\n",
       " 'natasha romanoff': ['natasha', 'black widow'],\n",
       " 'sam wilson': ['sam'],\n",
       " 'steve rogers': ['captain america', 'steve', 'cap'],\n",
       " 'collector': [],\n",
       " 'secretary ross': [],\n",
       " 'james rhodes': ['rhodey', 'war machine'],\n",
       " 'okoye': [],\n",
       " \"t'challa\": [\"king t'challa\"],\n",
       " 'bucky barnes': ['winter soldier', 'bucky'],\n",
       " 'eitri': [],\n",
       " 'nebula': ['memory nebula'],\n",
       " 'stonekeeper': [],\n",
       " 'red skull': [],\n",
       " 'shuri': [],\n",
       " 'jabari warriors': ['jabari'],\n",
       " \"m'baku\": [],\n",
       " 'dome control': [],\n",
       " 'nick fury': ['fury'],\n",
       " 'maria hill': ['hill'],\n",
       " 'dora milaje': ['dora'],\n",
       " 'captain marvel': [],\n",
       " 'kingsguard': [],\n",
       " 'scott lang': ['scott', 'antman', 'ant man'],\n",
       " 'infinity stones': ['tesseract', 'mind stone', 'stone']}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_lookup = {\n",
    "    \n",
    "'WinterSoldier' : [],\n",
    "'blackwidow' : [],\n",
    "'CaptainMarvel' : [],\n",
    "'Manntis' : [],\n",
    "'Ebony Maw': ['Maw', \"Ebony Maw's\"],\n",
    "'Thanos': [], \n",
    "'Thor': [], \n",
    "'captainamerica':[],\n",
    "'Loki': [], \n",
    "'Heimdall': [],\n",
    "'Proxima Midnight': [],\n",
    "'Stephen Strange': ['Strange', 'Dr Strange', 'Doctor Strange', \"Doctor Strange's\", 'Doc'],\n",
    "'Wong': [], \n",
    "'Bruce Banner': ['Hulk',  'Banner',  'Bruce'], \n",
    "'Tony Stark': [\"Iron Man's\", 'Iron Man', 'Stark', 'Tony'],\n",
    "'Pepper Potts': ['Pepper', 'Potts'], \n",
    "'F.R.I.D.A.Y.': [], \n",
    "'Peter Parker': ['Spiderman', 'Iron Spider', 'Peter'],\n",
    "'Ned Leeds': [],\n",
    "'School Bus Driver, Stan Lee': [], \n",
    "'Cull Obsidian': [\"Cull Obsidian's\"], \n",
    "'Peter Quill': ['Quill', 'Star Lord'], \n",
    "'Rocket': [], \n",
    "'Gamora': ['Young Gamora', 'Memory Gamora'], \n",
    "'Drax': [], \n",
    "'Mantis': [], \n",
    "'Groot': [], \n",
    "'Vision': [], \n",
    "'Wanda Maximoff': ['Wanda Maximoff', 'Wanda', 'Scarlet'], \n",
    "'Corvus Glaive': ['Corvus', \"Corvus Glaive's\", 'Glaive'], \n",
    "'Natasha Romanoff': ['Natasha', 'Black Widow'], \n",
    "'Sam Wilson': ['Sam'], \n",
    "'Steve Rogers': ['Captain America', 'Steve', 'Cap'], \n",
    "'Collector': [], \n",
    "'Secretary Ross': [], \n",
    "'James Rhodes': ['Rhodey', 'War machine'], \n",
    "'Okoye': [], \n",
    "\"T'Challa\": [\"King T'Challa\"], \n",
    "'Bucky Barnes': ['Winter Soldier', 'Bucky'], \n",
    "'Eitri': [], \n",
    "'Nebula': ['Memory Nebula'], \n",
    "'Stonekeeper': [], \n",
    "'Red Skull': [], \n",
    "'Shuri': [], \n",
    "'Jabari Warriors': ['Jabari'],\n",
    "\"M'Baku\": [],\n",
    "'Dome Control': [],\n",
    "'Nick Fury': ['Fury'], \n",
    "'Maria Hill': ['Hill'],\n",
    "'Dora Milaje': ['Dora'],\n",
    "'Captain Marvel': [],\n",
    "'Kingsguard': [],\n",
    "'Scott Lang': ['Scott', 'Antman', 'Ant man'],\n",
    "'Infinity stones': ['Tesseract',  'Mind Stone', 'Stone']\n",
    "}\n",
    "\n",
    "character_lookup =  {k.lower(): [x.lower() for x in v] for k, v in character_lookup.items()}\n",
    "\n",
    "character_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('wintersoldier', []), ('blackwidow', []), ('captainmarvel', []), ('manntis', []), ('ebony maw', ['maw', \"ebony maw's\"]), ('thanos', []), ('thor', []), ('captainamerica', []), ('loki', []), ('heimdall', []), ('proxima midnight', []), ('stephen strange', ['strange', 'dr strange', 'doctor strange', \"doctor strange's\", 'doc']), ('wong', []), ('bruce banner', ['hulk', 'banner', 'bruce']), ('tony stark', [\"iron man's\", 'iron man', 'stark', 'tony']), ('pepper potts', ['pepper', 'potts']), ('f.r.i.d.a.y.', []), ('peter parker', ['spiderman', 'iron spider', 'peter']), ('ned leeds', []), ('school bus driver, stan lee', []), ('cull obsidian', [\"cull obsidian's\"]), ('peter quill', ['quill', 'star lord']), ('rocket', []), ('gamora', ['young gamora', 'memory gamora']), ('drax', []), ('mantis', []), ('groot', []), ('vision', []), ('wanda maximoff', ['wanda maximoff', 'wanda', 'scarlet']), ('corvus glaive', ['corvus', \"corvus glaive's\", 'glaive']), ('natasha romanoff', ['natasha', 'black widow']), ('sam wilson', ['sam']), ('steve rogers', ['captain america', 'steve', 'cap']), ('collector', []), ('secretary ross', []), ('james rhodes', ['rhodey', 'war machine']), ('okoye', []), (\"t'challa\", [\"king t'challa\"]), ('bucky barnes', ['winter soldier', 'bucky']), ('eitri', []), ('nebula', ['memory nebula']), ('stonekeeper', []), ('red skull', []), ('shuri', []), ('jabari warriors', ['jabari']), (\"m'baku\", []), ('dome control', []), ('nick fury', ['fury']), ('maria hill', ['hill']), ('dora milaje', ['dora']), ('captain marvel', []), ('kingsguard', []), ('scott lang', ['scott', 'antman', 'ant man']), ('infinity stones', ['tesseract', 'mind stone', 'stone'])])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_lookup.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tweets_df['sentences'] = tweets_df['text'].map(sent_tokenize)\n",
    "tweets_df['tokens_sentences'] = tweets_df['sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "tweets_df['POS_tokens'] = tweets_df['tokens_sentences'].apply(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "tweets_df['tokens_sentences_lemmatized'] = tweets_df['POS_tokens'].apply(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "tweets_df['tokens'] = tweets_df['tokens_sentences_lemmatized'].apply(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "tweets_df['tokens'] = tweets_df['tokens'].apply(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'marvelstudios',\n",
       " 'star',\n",
       " 'avengersendgame',\n",
       " 'begin',\n",
       " 'arrive',\n",
       " 'world',\n",
       " 'premiere',\n",
       " 'http']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['tokens'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_names_in_dict(input_str_list):\n",
    "    list_of_found_chars = []\n",
    "    for i in range(len(input_str_list)):\n",
    "        list_item = input_str_list[i]\n",
    "        if character_lookup.get(list_item)!=None: \n",
    "            list_of_found_chars.append(list_item)\n",
    "        else: \n",
    "            pass\n",
    "    return list_of_found_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names_avengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[blackwidow, captainamerica]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[captainamerica]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>[captainamerica]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     names_avengers\n",
       "0                                []\n",
       "1      [blackwidow, captainamerica]\n",
       "2                                []\n",
       "3                                []\n",
       "4                  [captainamerica]\n",
       "...                             ...\n",
       "14995                            []\n",
       "14996                            []\n",
       "14997                            []\n",
       "14998              [captainamerica]\n",
       "14999                            []\n",
       "\n",
       "[15000 rows x 1 columns]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['names_avengers'] = tweets_df['tokens'].apply(lambda x: find_names_in_dict(x))\n",
    "\n",
    "DF_names_avengers = DataFrame( tweets_df['names_avengers'] )\n",
    "\n",
    "DF_names_avengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( DF_names_avengers, open('data/DF_names_avengers.pklz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/DF_names_avengers.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/DF_names_avengers.yaml\n",
    "\n",
    "The dataframe is saved in a pandas core dataframe. The names of all the avengers are extracted if they exist in the dictionary which is build from the internet.\n",
    "It is a pickle file so it can be opened with pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Create set of classifiers for logistic regression with different features for feature generation. \n",
    "use the following methods of feature generation\n",
    "\n",
    " - word tokenization with 1-gram, f\n",
    " - pos tokenization with extraction of all NP \n",
    " - pos tokenization with 2-gram bag of tokens for all NNP\n",
    " - 2-gram bag of characters for screenName features\n",
    " - polynomial features for presence of avenger character names \n",
    " \n",
    "Organize classes of transformers in various .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\n#AvengersEndgame https://t.co/LR9kFwfD5c'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn import *\n",
    "tweets_df = pd.read_csv('tweets.csv', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir set_of_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile set_of_classifiers/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD LOGIC 7_1\n",
    "\n",
    "## word tokenization with 1-gram, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweets_txt_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# init txt_df\n",
    "tweets_txt_df = tweets_df[['text']]\n",
    "tweets_txt_df.head()\n",
    "\n",
    "%store tweets_txt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/berend/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/berend/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/berend/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def clean_text(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ['RT']\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore'))\n",
    "    words = re.sub(r'[^\\w\\s]', '', text) \n",
    "    words = re.sub(r'(http.+)', '', words).split()\n",
    "    return ' '.join([word for word in words if word not in stopwords])\n",
    "\n",
    "tweets_df['text'] = tweets_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text as sk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           mrvelstan literally nobody AvengersEndgame\n",
       "1    agntecarter im emotional sorry 2014 x 2019 bla...\n",
       "2          saving bingo cards tomorrow AvengersEndgame\n",
       "3         HelloBoon Man AvengersEndgame ads everywhere\n",
       "4    Marvel We salute ChrisEvans CaptainAmerica Ave...\n",
       "5    MCU_Direct The first NONSPOILER AvengersEndgam...\n",
       "6    Renner4Real Ready rock excited avengersendgame...\n",
       "7    Avengers Were til end line WinterSoldier Aveng...\n",
       "8    Variety AvengersEndgame first reactions Most e...\n",
       "9         HelloBoon Man AvengersEndgame ads everywhere\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_txt_df = vectorizer.fit_transform(tweets_txt_df['text'])\n",
    "\n",
    "#vectorized_txt_df.shape, tweets_txt_df.shape, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Logic 7_2\n",
    "\n",
    "## pos tokenization with extraction of all NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from set_of_classifiers import py_2_word_tokenization_2gramNNP\n",
    "\n",
    "instance2 = py_2_word_tokenization_2gramNNP.word_tokenization_2gramNNP(tweets_df)\n",
    "\n",
    "instance2.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from set_of_classifiers import py_1_ngram_tokenization\n",
    "\n",
    "ngram = 2\n",
    "\n",
    "instance2 = py_1_ngram_tokenization.word_tokenization_ngram(tweets_df['text'],ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_postags = instance2.deliver_position_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def extract_NNP_from_postag(tweets_postags):\n",
    "    # function to test if something is a noun\n",
    "    is_proper_noun = lambda pos: pos == 'NNP'\n",
    "\n",
    "    proper_nouns_list=[]\n",
    "    for i in range(len(tweets_postags)):\n",
    "        proper_nouns = [word for (word, pos) in tweets_postags[i][0] if is_proper_noun(pos)] \n",
    "        proper_nouns_list.append(proper_nouns)\n",
    "    return proper_nouns_list\n",
    "\n",
    "\n",
    "extract_NNP_from_postag(tweets_postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LOGIC 7_3\n",
    "## pos tokenization with 2-gram bag of tokens for all NNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    mrvelstan nobody https //t.co/LR9kFwfD5c\n",
       "1       agntecarter i https //t.co/xcwkCMw18w\n",
       "2      bingo tomorrow https //t.co/d6For0jwRb\n",
       "Name: nn_tokens, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('NNP')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tweets_txt_df['tokens'] = tweets_txt_df['text'].apply(word_tokenize)\n",
    "tweets_txt_df['pos_tokens'] = tweets_txt_df['tokens'].apply(pos_tag)\n",
    "tweets_txt_df['nn_tokens'] = tweets_txt_df['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NN'])\n",
    "tweets_txt_df['nn_tokens'] = tweets_txt_df['nn_tokens'].apply(lambda x: ' '.join(x))\n",
    "tweets_txt_df['nn_tokens'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 3368), (15000, 4))"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_nn_df = vectorizer.fit_transform(tweets_txt_df['nn_tokens'])\n",
    "\n",
    "#vectorized_nn_df.shape, tweets_txt_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LOGIC 7_4\n",
    "## 2-gram bag of characters for screenName features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          AvengersEndgame\n",
       "1                                         \n",
       "2                                         \n",
       "3            HelloBoon Man AvengersEndgame\n",
       "4    Marvel CaptainAmerica AvengersEndgame\n",
       "Name: nnp_tokens, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tweets_df['tokens'] = tweets_df['text'].apply(word_tokenize)\n",
    "tweets_df['pos_tokens'] = tweets_df['tokens'].apply(pos_tag)\n",
    "tweets_df['nnp_tokens'] = tweets_df['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NNP'])\n",
    "tweets_df['nnp_tokens'] = tweets_df['nnp_tokens'].apply(lambda x: ' '.join(x))\n",
    "tweets_df['nnp_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "vectorized_nnp_df = vectorizer.fit_transform(tweets_df['nnp_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LOGIC 7_4\n",
    "## 2-gram bag of characters for screenName features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              DavidAc96\n",
       "1                NRmalaa\n",
       "2               jijitsuu\n",
       "3               SahapunB\n",
       "4            stella22_97\n",
       "              ...       \n",
       "14995          tommysboi\n",
       "14996    kimberleywithae\n",
       "14997         Gnanavel07\n",
       "14998         _moonljght\n",
       "14999    CaterinaCabrel1\n",
       "Name: screenName, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['screenName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 1271),\n",
       " ['00',\n",
       "  '01',\n",
       "  '02',\n",
       "  '03',\n",
       "  '04',\n",
       "  '05',\n",
       "  '06',\n",
       "  '07',\n",
       "  '08',\n",
       "  '09',\n",
       "  '0_',\n",
       "  '0a',\n",
       "  '0b',\n",
       "  '0c',\n",
       "  '0d',\n",
       "  '0f',\n",
       "  '0h',\n",
       "  '0i',\n",
       "  '0k',\n",
       "  '0l',\n",
       "  '0m',\n",
       "  '0n',\n",
       "  '0o',\n",
       "  '0p',\n",
       "  '0r',\n",
       "  '0s',\n",
       "  '0t',\n",
       "  '0u',\n",
       "  '0v',\n",
       "  '0w',\n",
       "  '0x',\n",
       "  '0y',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '1_',\n",
       "  '1a',\n",
       "  '1b',\n",
       "  '1c',\n",
       "  '1d',\n",
       "  '1e',\n",
       "  '1f',\n",
       "  '1h',\n",
       "  '1j',\n",
       "  '1k',\n",
       "  '1l',\n",
       "  '1m',\n",
       "  '1n',\n",
       "  '1p',\n",
       "  '1q',\n",
       "  '1r',\n",
       "  '1s',\n",
       "  '1t',\n",
       "  '1v',\n",
       "  '20',\n",
       "  '21',\n",
       "  '22',\n",
       "  '23',\n",
       "  '24',\n",
       "  '25',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '2_',\n",
       "  '2a',\n",
       "  '2b',\n",
       "  '2c',\n",
       "  '2d',\n",
       "  '2e',\n",
       "  '2f',\n",
       "  '2g',\n",
       "  '2i',\n",
       "  '2j',\n",
       "  '2k',\n",
       "  '2m',\n",
       "  '2n',\n",
       "  '2p',\n",
       "  '2q',\n",
       "  '2r',\n",
       "  '2s',\n",
       "  '2u',\n",
       "  '2v',\n",
       "  '2w',\n",
       "  '2y',\n",
       "  '2z',\n",
       "  '30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '3_',\n",
       "  '3a',\n",
       "  '3b',\n",
       "  '3c',\n",
       "  '3d',\n",
       "  '3e',\n",
       "  '3f',\n",
       "  '3h',\n",
       "  '3k',\n",
       "  '3l',\n",
       "  '3n',\n",
       "  '3p',\n",
       "  '3q',\n",
       "  '3r',\n",
       "  '3s',\n",
       "  '3t',\n",
       "  '3u',\n",
       "  '3v',\n",
       "  '3w',\n",
       "  '3y',\n",
       "  '3z',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '4_',\n",
       "  '4a',\n",
       "  '4c',\n",
       "  '4d',\n",
       "  '4e',\n",
       "  '4f',\n",
       "  '4g',\n",
       "  '4k',\n",
       "  '4l',\n",
       "  '4m',\n",
       "  '4n',\n",
       "  '4o',\n",
       "  '4p',\n",
       "  '4r',\n",
       "  '4s',\n",
       "  '4t',\n",
       "  '4u',\n",
       "  '4x',\n",
       "  '4y',\n",
       "  '4z',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '5_',\n",
       "  '5a',\n",
       "  '5b',\n",
       "  '5c',\n",
       "  '5d',\n",
       "  '5e',\n",
       "  '5f',\n",
       "  '5g',\n",
       "  '5h',\n",
       "  '5i',\n",
       "  '5k',\n",
       "  '5m',\n",
       "  '5n',\n",
       "  '5p',\n",
       "  '5r',\n",
       "  '5s',\n",
       "  '5t',\n",
       "  '5v',\n",
       "  '5y',\n",
       "  '5z',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '6_',\n",
       "  '6a',\n",
       "  '6b',\n",
       "  '6d',\n",
       "  '6f',\n",
       "  '6h',\n",
       "  '6i',\n",
       "  '6j',\n",
       "  '6k',\n",
       "  '6l',\n",
       "  '6m',\n",
       "  '6p',\n",
       "  '6r',\n",
       "  '6s',\n",
       "  '6t',\n",
       "  '6v',\n",
       "  '6x',\n",
       "  '6z',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '7_',\n",
       "  '7a',\n",
       "  '7b',\n",
       "  '7c',\n",
       "  '7d',\n",
       "  '7g',\n",
       "  '7i',\n",
       "  '7k',\n",
       "  '7l',\n",
       "  '7m',\n",
       "  '7n',\n",
       "  '7p',\n",
       "  '7r',\n",
       "  '7s',\n",
       "  '7t',\n",
       "  '7v',\n",
       "  '7z',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '8_',\n",
       "  '8a',\n",
       "  '8c',\n",
       "  '8g',\n",
       "  '8h',\n",
       "  '8i',\n",
       "  '8k',\n",
       "  '8m',\n",
       "  '8n',\n",
       "  '8o',\n",
       "  '8r',\n",
       "  '8s',\n",
       "  '8t',\n",
       "  '8v',\n",
       "  '8w',\n",
       "  '8x',\n",
       "  '90',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99',\n",
       "  '9_',\n",
       "  '9a',\n",
       "  '9b',\n",
       "  '9c',\n",
       "  '9d',\n",
       "  '9e',\n",
       "  '9f',\n",
       "  '9g',\n",
       "  '9k',\n",
       "  '9l',\n",
       "  '9m',\n",
       "  '9n',\n",
       "  '9p',\n",
       "  '9s',\n",
       "  '9t',\n",
       "  '9v',\n",
       "  '9x',\n",
       "  '9z',\n",
       "  '_0',\n",
       "  '_1',\n",
       "  '_2',\n",
       "  '_3',\n",
       "  '_4',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '_9',\n",
       "  '__',\n",
       "  '_a',\n",
       "  '_b',\n",
       "  '_c',\n",
       "  '_d',\n",
       "  '_e',\n",
       "  '_f',\n",
       "  '_g',\n",
       "  '_h',\n",
       "  '_i',\n",
       "  '_j',\n",
       "  '_k',\n",
       "  '_l',\n",
       "  '_m',\n",
       "  '_n',\n",
       "  '_o',\n",
       "  '_p',\n",
       "  '_q',\n",
       "  '_r',\n",
       "  '_s',\n",
       "  '_t',\n",
       "  '_u',\n",
       "  '_v',\n",
       "  '_w',\n",
       "  '_x',\n",
       "  '_y',\n",
       "  '_z',\n",
       "  'a0',\n",
       "  'a1',\n",
       "  'a2',\n",
       "  'a3',\n",
       "  'a4',\n",
       "  'a5',\n",
       "  'a6',\n",
       "  'a7',\n",
       "  'a8',\n",
       "  'a9',\n",
       "  'a_',\n",
       "  'aa',\n",
       "  'ab',\n",
       "  'ac',\n",
       "  'ad',\n",
       "  'ae',\n",
       "  'af',\n",
       "  'ag',\n",
       "  'ah',\n",
       "  'ai',\n",
       "  'aj',\n",
       "  'ak',\n",
       "  'al',\n",
       "  'am',\n",
       "  'an',\n",
       "  'ao',\n",
       "  'ap',\n",
       "  'aq',\n",
       "  'ar',\n",
       "  'as',\n",
       "  'at',\n",
       "  'au',\n",
       "  'av',\n",
       "  'aw',\n",
       "  'ax',\n",
       "  'ay',\n",
       "  'az',\n",
       "  'b0',\n",
       "  'b1',\n",
       "  'b2',\n",
       "  'b3',\n",
       "  'b4',\n",
       "  'b5',\n",
       "  'b6',\n",
       "  'b9',\n",
       "  'b_',\n",
       "  'ba',\n",
       "  'bb',\n",
       "  'bc',\n",
       "  'bd',\n",
       "  'be',\n",
       "  'bf',\n",
       "  'bg',\n",
       "  'bh',\n",
       "  'bi',\n",
       "  'bj',\n",
       "  'bk',\n",
       "  'bl',\n",
       "  'bm',\n",
       "  'bn',\n",
       "  'bo',\n",
       "  'bp',\n",
       "  'bq',\n",
       "  'br',\n",
       "  'bs',\n",
       "  'bt',\n",
       "  'bu',\n",
       "  'bv',\n",
       "  'bw',\n",
       "  'bx',\n",
       "  'by',\n",
       "  'bz',\n",
       "  'c0',\n",
       "  'c1',\n",
       "  'c2',\n",
       "  'c3',\n",
       "  'c4',\n",
       "  'c6',\n",
       "  'c7',\n",
       "  'c8',\n",
       "  'c9',\n",
       "  'c_',\n",
       "  'ca',\n",
       "  'cb',\n",
       "  'cc',\n",
       "  'cd',\n",
       "  'ce',\n",
       "  'cf',\n",
       "  'cg',\n",
       "  'ch',\n",
       "  'ci',\n",
       "  'cj',\n",
       "  'ck',\n",
       "  'cl',\n",
       "  'cm',\n",
       "  'cn',\n",
       "  'co',\n",
       "  'cp',\n",
       "  'cq',\n",
       "  'cr',\n",
       "  'cs',\n",
       "  'ct',\n",
       "  'cu',\n",
       "  'cv',\n",
       "  'cw',\n",
       "  'cx',\n",
       "  'cy',\n",
       "  'cz',\n",
       "  'd0',\n",
       "  'd1',\n",
       "  'd2',\n",
       "  'd3',\n",
       "  'd4',\n",
       "  'd5',\n",
       "  'd6',\n",
       "  'd7',\n",
       "  'd8',\n",
       "  'd9',\n",
       "  'd_',\n",
       "  'da',\n",
       "  'db',\n",
       "  'dc',\n",
       "  'dd',\n",
       "  'de',\n",
       "  'df',\n",
       "  'dg',\n",
       "  'dh',\n",
       "  'di',\n",
       "  'dj',\n",
       "  'dk',\n",
       "  'dl',\n",
       "  'dm',\n",
       "  'dn',\n",
       "  'do',\n",
       "  'dp',\n",
       "  'dq',\n",
       "  'dr',\n",
       "  'ds',\n",
       "  'dt',\n",
       "  'du',\n",
       "  'dv',\n",
       "  'dw',\n",
       "  'dx',\n",
       "  'dy',\n",
       "  'dz',\n",
       "  'e0',\n",
       "  'e1',\n",
       "  'e2',\n",
       "  'e3',\n",
       "  'e4',\n",
       "  'e5',\n",
       "  'e6',\n",
       "  'e7',\n",
       "  'e8',\n",
       "  'e9',\n",
       "  'e_',\n",
       "  'ea',\n",
       "  'eb',\n",
       "  'ec',\n",
       "  'ed',\n",
       "  'ee',\n",
       "  'ef',\n",
       "  'eg',\n",
       "  'eh',\n",
       "  'ei',\n",
       "  'ej',\n",
       "  'ek',\n",
       "  'el',\n",
       "  'em',\n",
       "  'en',\n",
       "  'eo',\n",
       "  'ep',\n",
       "  'eq',\n",
       "  'er',\n",
       "  'es',\n",
       "  'et',\n",
       "  'eu',\n",
       "  'ev',\n",
       "  'ew',\n",
       "  'ex',\n",
       "  'ey',\n",
       "  'ez',\n",
       "  'f0',\n",
       "  'f1',\n",
       "  'f2',\n",
       "  'f3',\n",
       "  'f4',\n",
       "  'f6',\n",
       "  'f7',\n",
       "  'f9',\n",
       "  'f_',\n",
       "  'fa',\n",
       "  'fb',\n",
       "  'fc',\n",
       "  'fd',\n",
       "  'fe',\n",
       "  'ff',\n",
       "  'fg',\n",
       "  'fh',\n",
       "  'fi',\n",
       "  'fj',\n",
       "  'fk',\n",
       "  'fl',\n",
       "  'fm',\n",
       "  'fn',\n",
       "  'fo',\n",
       "  'fp',\n",
       "  'fq',\n",
       "  'fr',\n",
       "  'fs',\n",
       "  'ft',\n",
       "  'fu',\n",
       "  'fv',\n",
       "  'fw',\n",
       "  'fx',\n",
       "  'fy',\n",
       "  'fz',\n",
       "  'g0',\n",
       "  'g1',\n",
       "  'g2',\n",
       "  'g3',\n",
       "  'g4',\n",
       "  'g5',\n",
       "  'g6',\n",
       "  'g7',\n",
       "  'g8',\n",
       "  'g9',\n",
       "  'g_',\n",
       "  'ga',\n",
       "  'gb',\n",
       "  'gc',\n",
       "  'gd',\n",
       "  'ge',\n",
       "  'gf',\n",
       "  'gg',\n",
       "  'gh',\n",
       "  'gi',\n",
       "  'gj',\n",
       "  'gk',\n",
       "  'gl',\n",
       "  'gm',\n",
       "  'gn',\n",
       "  'go',\n",
       "  'gp',\n",
       "  'gq',\n",
       "  'gr',\n",
       "  'gs',\n",
       "  'gt',\n",
       "  'gu',\n",
       "  'gv',\n",
       "  'gw',\n",
       "  'gx',\n",
       "  'gy',\n",
       "  'gz',\n",
       "  'h0',\n",
       "  'h1',\n",
       "  'h2',\n",
       "  'h3',\n",
       "  'h4',\n",
       "  'h5',\n",
       "  'h6',\n",
       "  'h7',\n",
       "  'h8',\n",
       "  'h9',\n",
       "  'h_',\n",
       "  'ha',\n",
       "  'hb',\n",
       "  'hc',\n",
       "  'hd',\n",
       "  'he',\n",
       "  'hf',\n",
       "  'hg',\n",
       "  'hh',\n",
       "  'hi',\n",
       "  'hj',\n",
       "  'hk',\n",
       "  'hl',\n",
       "  'hm',\n",
       "  'hn',\n",
       "  'ho',\n",
       "  'hp',\n",
       "  'hq',\n",
       "  'hr',\n",
       "  'hs',\n",
       "  'ht',\n",
       "  'hu',\n",
       "  'hv',\n",
       "  'hw',\n",
       "  'hx',\n",
       "  'hy',\n",
       "  'hz',\n",
       "  'i0',\n",
       "  'i1',\n",
       "  'i2',\n",
       "  'i3',\n",
       "  'i4',\n",
       "  'i5',\n",
       "  'i6',\n",
       "  'i7',\n",
       "  'i8',\n",
       "  'i9',\n",
       "  'i_',\n",
       "  'ia',\n",
       "  'ib',\n",
       "  'ic',\n",
       "  'id',\n",
       "  'ie',\n",
       "  'if',\n",
       "  'ig',\n",
       "  'ih',\n",
       "  'ii',\n",
       "  'ij',\n",
       "  'ik',\n",
       "  'il',\n",
       "  'im',\n",
       "  'in',\n",
       "  'io',\n",
       "  'ip',\n",
       "  'iq',\n",
       "  'ir',\n",
       "  'is',\n",
       "  'it',\n",
       "  'iu',\n",
       "  'iv',\n",
       "  'iw',\n",
       "  'ix',\n",
       "  'iy',\n",
       "  'iz',\n",
       "  'j0',\n",
       "  'j1',\n",
       "  'j2',\n",
       "  'j3',\n",
       "  'j4',\n",
       "  'j5',\n",
       "  'j6',\n",
       "  'j7',\n",
       "  'j9',\n",
       "  'j_',\n",
       "  'ja',\n",
       "  'jb',\n",
       "  'jc',\n",
       "  'jd',\n",
       "  'je',\n",
       "  'jf',\n",
       "  'jg',\n",
       "  'jh',\n",
       "  'ji',\n",
       "  'jj',\n",
       "  'jk',\n",
       "  'jl',\n",
       "  'jm',\n",
       "  'jn',\n",
       "  'jo',\n",
       "  'jp',\n",
       "  'jq',\n",
       "  'jr',\n",
       "  'js',\n",
       "  'jt',\n",
       "  'ju',\n",
       "  'jv',\n",
       "  'jw',\n",
       "  'jx',\n",
       "  'jy',\n",
       "  'jz',\n",
       "  'k0',\n",
       "  'k1',\n",
       "  'k2',\n",
       "  'k3',\n",
       "  'k4',\n",
       "  'k5',\n",
       "  'k6',\n",
       "  'k7',\n",
       "  'k8',\n",
       "  'k9',\n",
       "  'k_',\n",
       "  'ka',\n",
       "  'kb',\n",
       "  'kc',\n",
       "  'kd',\n",
       "  'ke',\n",
       "  'kf',\n",
       "  'kg',\n",
       "  'kh',\n",
       "  'ki',\n",
       "  'kj',\n",
       "  'kk',\n",
       "  'kl',\n",
       "  'km',\n",
       "  'kn',\n",
       "  'ko',\n",
       "  'kp',\n",
       "  'kq',\n",
       "  'kr',\n",
       "  'ks',\n",
       "  'kt',\n",
       "  'ku',\n",
       "  'kv',\n",
       "  'kw',\n",
       "  'kx',\n",
       "  'ky',\n",
       "  'kz',\n",
       "  'l0',\n",
       "  'l1',\n",
       "  'l2',\n",
       "  'l3',\n",
       "  'l4',\n",
       "  'l5',\n",
       "  'l6',\n",
       "  'l7',\n",
       "  'l8',\n",
       "  'l9',\n",
       "  'l_',\n",
       "  'la',\n",
       "  'lb',\n",
       "  'lc',\n",
       "  'ld',\n",
       "  'le',\n",
       "  'lf',\n",
       "  'lg',\n",
       "  'lh',\n",
       "  'li',\n",
       "  'lj',\n",
       "  'lk',\n",
       "  'll',\n",
       "  'lm',\n",
       "  'ln',\n",
       "  'lo',\n",
       "  'lp',\n",
       "  'lq',\n",
       "  'lr',\n",
       "  'ls',\n",
       "  'lt',\n",
       "  'lu',\n",
       "  'lv',\n",
       "  'lw',\n",
       "  'lx',\n",
       "  'ly',\n",
       "  'lz',\n",
       "  'm0',\n",
       "  'm1',\n",
       "  'm2',\n",
       "  'm3',\n",
       "  'm4',\n",
       "  'm5',\n",
       "  'm6',\n",
       "  'm7',\n",
       "  'm8',\n",
       "  'm9',\n",
       "  'm_',\n",
       "  'ma',\n",
       "  'mb',\n",
       "  'mc',\n",
       "  'md',\n",
       "  'me',\n",
       "  'mf',\n",
       "  'mg',\n",
       "  'mh',\n",
       "  'mi',\n",
       "  'mj',\n",
       "  'mk',\n",
       "  'ml',\n",
       "  'mm',\n",
       "  'mn',\n",
       "  'mo',\n",
       "  'mp',\n",
       "  'mq',\n",
       "  'mr',\n",
       "  'ms',\n",
       "  'mt',\n",
       "  'mu',\n",
       "  'mv',\n",
       "  'mw',\n",
       "  'mx',\n",
       "  'my',\n",
       "  'mz',\n",
       "  'n0',\n",
       "  'n1',\n",
       "  'n2',\n",
       "  'n3',\n",
       "  'n4',\n",
       "  'n5',\n",
       "  'n6',\n",
       "  'n7',\n",
       "  'n8',\n",
       "  'n9',\n",
       "  'n_',\n",
       "  'na',\n",
       "  'nb',\n",
       "  'nc',\n",
       "  'nd',\n",
       "  'ne',\n",
       "  'nf',\n",
       "  'ng',\n",
       "  'nh',\n",
       "  'ni',\n",
       "  'nj',\n",
       "  'nk',\n",
       "  'nl',\n",
       "  'nm',\n",
       "  'nn',\n",
       "  'no',\n",
       "  'np',\n",
       "  'nq',\n",
       "  'nr',\n",
       "  'ns',\n",
       "  'nt',\n",
       "  'nu',\n",
       "  'nv',\n",
       "  'nw',\n",
       "  'nx',\n",
       "  'ny',\n",
       "  'nz',\n",
       "  'o0',\n",
       "  'o1',\n",
       "  'o2',\n",
       "  'o3',\n",
       "  'o4',\n",
       "  'o5',\n",
       "  'o6',\n",
       "  'o7',\n",
       "  'o8',\n",
       "  'o9',\n",
       "  'o_',\n",
       "  'oa',\n",
       "  'ob',\n",
       "  'oc',\n",
       "  'od',\n",
       "  'oe',\n",
       "  'of',\n",
       "  'og',\n",
       "  'oh',\n",
       "  'oi',\n",
       "  'oj',\n",
       "  'ok',\n",
       "  'ol',\n",
       "  'om',\n",
       "  'on',\n",
       "  'oo',\n",
       "  'op',\n",
       "  'oq',\n",
       "  'or',\n",
       "  'os',\n",
       "  'ot',\n",
       "  'ou',\n",
       "  'ov',\n",
       "  'ow',\n",
       "  'ox',\n",
       "  'oy',\n",
       "  'oz',\n",
       "  'p0',\n",
       "  'p1',\n",
       "  'p2',\n",
       "  'p3',\n",
       "  'p5',\n",
       "  'p6',\n",
       "  'p7',\n",
       "  'p8',\n",
       "  'p9',\n",
       "  'p_',\n",
       "  'pa',\n",
       "  'pb',\n",
       "  'pc',\n",
       "  'pd',\n",
       "  'pe',\n",
       "  'pf',\n",
       "  'pg',\n",
       "  'ph',\n",
       "  'pi',\n",
       "  'pj',\n",
       "  'pk',\n",
       "  'pl',\n",
       "  'pm',\n",
       "  'pn',\n",
       "  'po',\n",
       "  'pp',\n",
       "  'pq',\n",
       "  'pr',\n",
       "  'ps',\n",
       "  'pt',\n",
       "  'pu',\n",
       "  'pv',\n",
       "  'pw',\n",
       "  'px',\n",
       "  'py',\n",
       "  'pz',\n",
       "  'q1',\n",
       "  'q2',\n",
       "  'q3',\n",
       "  'q8',\n",
       "  'q9',\n",
       "  'q_',\n",
       "  'qa',\n",
       "  'qb',\n",
       "  'qc',\n",
       "  'qd',\n",
       "  'qe',\n",
       "  'qg',\n",
       "  'qh',\n",
       "  'qi',\n",
       "  'qk',\n",
       "  'ql',\n",
       "  'qm',\n",
       "  'qn',\n",
       "  'qo',\n",
       "  'qq',\n",
       "  'qr',\n",
       "  'qs',\n",
       "  'qt',\n",
       "  'qu',\n",
       "  'qv',\n",
       "  'qw',\n",
       "  'qx',\n",
       "  'qy',\n",
       "  'qz',\n",
       "  'r0',\n",
       "  'r1',\n",
       "  'r2',\n",
       "  'r3',\n",
       "  'r4',\n",
       "  'r5',\n",
       "  'r6',\n",
       "  'r7',\n",
       "  'r8',\n",
       "  'r9',\n",
       "  'r_',\n",
       "  'ra',\n",
       "  'rb',\n",
       "  'rc',\n",
       "  'rd',\n",
       "  're',\n",
       "  'rf',\n",
       "  'rg',\n",
       "  'rh',\n",
       "  'ri',\n",
       "  'rj',\n",
       "  'rk',\n",
       "  'rl',\n",
       "  'rm',\n",
       "  'rn',\n",
       "  'ro',\n",
       "  'rp',\n",
       "  'rq',\n",
       "  'rr',\n",
       "  'rs',\n",
       "  'rt',\n",
       "  'ru',\n",
       "  'rv',\n",
       "  'rw',\n",
       "  'rx',\n",
       "  'ry',\n",
       "  'rz',\n",
       "  's0',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6',\n",
       "  's7',\n",
       "  's8',\n",
       "  's9',\n",
       "  's_',\n",
       "  'sa',\n",
       "  'sb',\n",
       "  'sc',\n",
       "  'sd',\n",
       "  'se',\n",
       "  'sf',\n",
       "  'sg',\n",
       "  'sh',\n",
       "  ...])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "vectorized_screenName_df = vectorizer.fit_transform(tweets_df['screenName'])\n",
    "\n",
    "vectorized_screenName_df.shape, vectorizer.get_feature_names(), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LOGIC 7_5\n",
    "## polynomial features for presence of avenger character names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%store -r tweets_df\n",
    "tweets_txt_df = tweets_df[['text']]\n",
    "\n",
    "heroes = [\"Iron Man\", \"Thor\", \"Captain America\", \"Captain Marvel\", \"Black Widow\", \"Hawkeye\", \"Hulk\", \"Vision\", \"Scarlet Witch\", \"War Machine\", \"Falcon\", \"StarLord\", \"Rocket Raccoon\", \"Groot\", \"Gamora\", \"Drax\", \"Mantis\", \"Nebula\", \"Doctor Strange\", \"Wong\", \"Spider-Man\", \"Spiderman\", \"Winter Soldier\", \"Heimdall\", \"Black Panther\", \"Okoye\", \"Shuri\", \"M’Baku\", \"Eitiri\", \"Nick Fury\", \"Maria Hill\", \"Pepper Potts\", \"William “Thunderbolt” Ross\", \"Ned\", \"Thanos\", \"Loki\", \"the Collector\", \"Cull Obsidian\", \"Ebony Maw\", \"Proxima Midnight\", \"Corvus Glaive\", \"Red Skull\", \"The Wasp\"]\n",
    "heroes = [h.lower().replace(\" \", \"\") for h in heroes]\n",
    "\n",
    "cv = CountVectorizer(vocabulary=heroes)\n",
    "\n",
    "heroes_count_df = pd.DataFrame.sparse.from_spmatrix(cv.fit_transform(tweets_txt_df['text']), \n",
    "                   tweets_txt_df.index,\n",
    "                   cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_count_arr = heroes_count_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "transformed_df = poly.fit_transform(heroes_count_arr)\n",
    "\n",
    "transformed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments - EX8\n",
    "# Experiments - EX8\n",
    "# Experiments - EX8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Apply PCA dimensionality reduction and LogisticRegression to predict Target, construct pipelines for all transformers from Exercise 5 and 7 (name them exp_[your name acronim]\\_6_1, ... exp_[your name acronim]\\_6_...) implement them as custom classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 (EX8) - Polynomial feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\n",
      "mkdir: cannot create directory ‘experiments/exp_5_1’: File exists\n",
      "mkdir: cannot create directory ‘experiments/exp_5_1/result’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_5_1\n",
    "!mkdir experiments/exp_5_1/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/config.yaml\n",
    "\n",
    "description: This experiment takes numerical features, generates new features using PolynomialFeatures, reduces features with PCA and classifies the data using Logistic Regression.\n",
    "\n",
    "features: ['favoriteCount', 'number_of_words', 'number_of_chars', 'number_of_hashtags','Bin_High_Retweet_Count']\n",
    "target: ['Bin_High_Retweet_Count']\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - target\n",
    "    - pca_components\n",
    "    - poly_degree\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None\n",
    "poly_degree: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_5_1(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        return None\n",
    "    \n",
    "    def number_of_words(self, sentence):\n",
    "        sentence_array = word_tokenize(sentence)\n",
    "        words = [word for word in sentence_array if word.isalpha()]\n",
    "        return len(words)\n",
    "    \n",
    "    def number_of_chars(self, sentence):\n",
    "        return len(sentence)\n",
    "\n",
    "    def number_of_hashtags(self, sentence):\n",
    "        return len(re.findall('#\\w+', sentence))\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        X['number_of_words'] = X['text'].apply(self.number_of_words)\n",
    "        X['number_of_chars'] = X['text'].apply(self.number_of_chars)\n",
    "        X['number_of_hashtags'] = X['text'].apply(self.number_of_hashtags)\n",
    "\n",
    "        poly = preprocessing.PolynomialFeatures(degree=3)\n",
    "        transformed_df = poly.fit_transform(X.drop('text', axis=1))\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_5_1.transformer import Transformer_5_1\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('split.pickle', 'rb'))\n",
    "\n",
    "features = ['text', 'favoriteCount']\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_5_1/config.yaml', Transformer_5_1, 'experiments/exp_5_1/result/')\n",
    "classifier.fit(X_train[features], y_train[target])\n",
    "y_pred = classifier.predict(X_test[features])\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_5_1/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/berend/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/berend/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[nltk_data] Downloading package punkt to /home/berend/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 34, 'accuracy': 0.6696666666666666, 'precision': 0.5534426229508197, 'recall': 0.7313691507798961, 'mutual_info_score': 0.09402586066911897}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_5_1/result/result_03-Nov-2020-15-26-50.yaml\n",
      "CPU times: user 2.82 s, sys: 16.4 ms, total: 2.84 s\n",
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_5_1/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7_1 (ex8) - word tokenization with 1-gram, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_1\n",
    "!mkdir experiments/exp_7_1/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_1/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/config.yaml\n",
    "\n",
    "description: This experiment uses 1-gram tokenization and logistic regression for classification. \n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance greater than .9\n",
    "    \n",
    "features: 1-gram tokenization of text\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - target\n",
    "    - pca_components\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_1/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/transformer.py\n",
    "### Custom Transformer for 1-gram tokenization\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_1(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = X.apply(self.clean_text)\n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X)\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/transformer.py\n",
    "### Custom Transformer for 1-gram tokenization\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_1(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = X.apply(self.clean_text)\n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X)\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_1.transformer import Transformer_7_1\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_1/config.yaml', Transformer_7_1, 'experiments/exp_7_1/result/')\n",
    "classifier.fit(X_train[features], y_train[target])\n",
    "y_pred = classifier.predict(X_test[features])\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_1/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berend/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 633, 'accuracy': 0.5626666666666666, 'precision': 0.620327868852459, 'recall': 0.5634306134603931, 'mutual_info_score': 0.010982605943444043}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_1/result/result_03-Nov-2020-15-29-20.yaml\n",
      "CPU times: user 5min 49s, sys: 3.48 s, total: 5min 52s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_1/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7_2 (ex8) - pos tokenization with extraction of all NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_2\n",
    "!mkdir experiments/exp_7_2/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_2/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_2/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/config.yaml\n",
    "\n",
    "description: This experiment uses 2-gram tokenization and logistic regression for classification. With extraction of NP\n",
    "    \n",
    "features: 1-gram tokenization of text\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - target\n",
    "    - pca_components\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_2/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_2(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('tagsets')\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):        \n",
    "        X['text'] = X['text'].apply(self.clean_text)\n",
    "        X['tokens'] = X['text'].apply(word_tokenize)\n",
    "        X['pos_tokens'] = X['tokens'].apply(pos_tag)\n",
    "        X['nn_tokens'] = X['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NN'])\n",
    "        X['nn_tokens'] = X['nn_tokens'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X['nn_tokens'])\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_2/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_2.transformer import Transformer_7_2\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_2/config.yaml', Transformer_7_2, 'experiments/exp_7_2/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_2/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 523, 'accuracy': 0.6376666666666667, 'precision': 0.4734426229508197, 'recall': 0.7176938369781312, 'mutual_info_score': 0.06798674078872291}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_2/result/result_03-Nov-2020-15-42-49.yaml\n",
      "CPU times: user 29.3 s, sys: 572 ms, total: 29.9 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_2/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7_3 (ex8) - pos tokenization with 2-gram bag of tokens for all NNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\n",
      "mkdir: cannot create directory ‘experiments/exp_7_3’: File exists\n",
      "mkdir: cannot create directory ‘experiments/exp_7_3/result’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_3\n",
    "!mkdir experiments/exp_7_3/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_3/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_3/config.yaml\n",
    "\n",
    "description: This experiment uses pos tokenization with extraction of NNP (2-gram) and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: pos tokenization with extraction of NN\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l1\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_3/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_3/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_3(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('tagsets')\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):        \n",
    "        X['text'] = X['text'].apply(self.clean_text)\n",
    "        X['tokens'] = X['text'].apply(word_tokenize)\n",
    "        X['pos_tokens'] = X['tokens'].apply(pos_tag)\n",
    "        X['nnp_tokens'] = X['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NNP'])\n",
    "        X['nnp_tokens'] = X['nnp_tokens'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X['nnp_tokens'])\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_3/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_3/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_3.transformer import Transformer_7_3\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_3/config.yaml', Transformer_7_3, 'experiments/exp_7_3/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_3/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "/home/berend/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 721, 'accuracy': 0.6913333333333334, 'precision': 0.6544262295081967, 'recall': 0.7143879742304939, 'mutual_info_score': 0.10972442446934183}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_3/result/result_03-Nov-2020-15-47-01.yaml\n",
      "CPU times: user 1min 13s, sys: 816 ms, total: 1min 14s\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_3/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7_4  (ex8) - 2-gram bag of characters for screenName features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_4\n",
    "!mkdir experiments/exp_7_4/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_4/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/config.yaml\n",
    "\n",
    "description: This experiment uses 2-gram bag of chars for screenName feature and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: characters paired 2 by 2\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l1\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_4/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_4(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('tagsets')\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "        transformer_df = vectorizer.fit_transform(X['screenName'])\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_4/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_4.transformer import Transformer_7_4\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./split.pickle', 'rb'))\n",
    "\n",
    "features = 'screenName'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_4/config.yaml', Transformer_7_4, 'experiments/exp_7_4/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_4/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_4.transformer import Transformer_7_4\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./split.pickle', 'rb'))\n",
    "\n",
    "features = 'screenName'\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_4/config.yaml', Transformer_7_4, 'experiments/exp_7_4/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_4/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/berend/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 454, 'accuracy': 0.49866666666666665, 'precision': 0.4963934426229508, 'recall': 0.507032819825854, 'mutual_info_score': -0.00023581555337273138}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_4/result/result_03-Nov-2020-15-47-40.yaml\n",
      "CPU times: user 19.2 s, sys: 436 ms, total: 19.6 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_4/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7_5 (ex8) - polynomial features for presence of avenger character names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘experiments’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_5\n",
    "!mkdir experiments/exp_7_5/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_5/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/config.yaml\n",
    "\n",
    "description: This experiment uses polynomial features for presence of hero name in text feature and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: polynomial features of dummy variables\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiments/exp_7_5/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_5(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = X[['text']]\n",
    "\n",
    "        heroes = [\"Iron Man\", \"Thor\", \"Captain America\", \"Captain Marvel\", \"Black Widow\", \"Hawkeye\", \"Hulk\", \"Vision\", \"Scarlet Witch\", \"War Machine\", \"Falcon\", \"StarLord\", \"Rocket Raccoon\", \"Groot\", \"Gamora\", \"Drax\", \"Mantis\", \"Nebula\", \"Doctor Strange\", \"Wong\", \"Spider-Man\", \"Spiderman\", \"Winter Soldier\", \"Heimdall\", \"Black Panther\", \"Okoye\", \"Shuri\", \"M’Baku\", \"Eitiri\", \"Nick Fury\", \"Maria Hill\", \"Pepper Potts\", \"William “Thunderbolt” Ross\", \"Ned\", \"Thanos\", \"Loki\", \"the Collector\", \"Cull Obsidian\", \"Ebony Maw\", \"Proxima Midnight\", \"Corvus Glaive\", \"Red Skull\", \"The Wasp\"]\n",
    "        heroes = [h.lower().replace(\" \", \"\") for h in heroes]\n",
    "\n",
    "        cv = CountVectorizer(vocabulary=heroes)\n",
    "\n",
    "        heroes_count_df = pd.DataFrame.sparse.from_spmatrix(cv.fit_transform(X['text']), \n",
    "                           X.index,\n",
    "                           cv.get_feature_names())\n",
    "        \n",
    "        heroes_count_arr = heroes_count_df.to_numpy()\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=3)\n",
    "        transformed_df = poly.fit_transform(heroes_count_arr)\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_5/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_5.transformer import Transformer_7_5\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'Bin_High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_5/config.yaml', Transformer_7_5, 'experiments/exp_7_5/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_5/config.yaml:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Python for Data Science/Homework_W2/experiments/exp_7_5/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticCustomClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'experiments/exp_7_5/config.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer_7_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'experiments/exp_7_5/result/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python for Data Science/Homework_W2/experiments/base/classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mtransformed_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python for Data Science/Homework_W2/experiments/base/classifier.py\u001b[0m in \u001b[0;36mfeature_transformation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtransformed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# select components with sum variance > 0.9 to determine the n_components var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0;31m# flip eigenvectors' sign to enforce deterministic output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python_for_DS/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[1;32m    126\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 4min 44s, sys: 35.2 s, total: 1h 5min 19s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_5/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rest of exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: Split dataset to train and test (it is up to you which features you will include in it) and store it data folder along with .yaml description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/config.yaml\n",
    "\n",
    "Description: Split for news_tweets\n",
    "\n",
    "test_size = 0.2\n",
    "random_state: 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>tokens_sentences</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tweettokens</th>\n",
       "      <th>names_avengers</th>\n",
       "      <th>POS_tokens</th>\n",
       "      <th>tokens_sentences_lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tokens</th>\n",
       "      <th>nnp_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mrvelstan literally nobody AvengersEndgame</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[RT, @, mrvelstan, :, literally, nobody, :, m...</td>\n",
       "      <td>[RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[(RT, NNP), (@, NNP), (mrvelstan, NN), (:, :)...</td>\n",
       "      <td>[[RT, @, mrvelstan, :, literally, nobody, :, m...</td>\n",
       "      <td>[mrvelstan, literally, nobody, AvengersEndgame]</td>\n",
       "      <td>[(mrvelstan, NNS), (literally, RB), (nobody, V...</td>\n",
       "      <td>AvengersEndgame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>agntecarter im emotional sorry 2014 x 2019 bla...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[RT, @, agntecarter, :, i, ’, m, emotional, ,...</td>\n",
       "      <td>[RT @agntecarter: i’m emotional, sorry!!, 2014...</td>\n",
       "      <td>None</td>\n",
       "      <td>[blackwidow, captainamerica]</td>\n",
       "      <td>[[(RT, NNP), (@, NNP), (agntecarter, NN), (:, ...</td>\n",
       "      <td>[[RT, @, agntecarter, :, i, ’, m, emotional, ,...</td>\n",
       "      <td>[agntecarter, im, emotional, sorry, 2014, x, 2...</td>\n",
       "      <td>[(agntecarter, RB), (im, JJ), (emotional, JJ),...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving bingo cards tomorrow AvengersEndgame</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[saving, these, bingo, cards, for, tomorrow, ...</td>\n",
       "      <td>[saving these bingo cards for tomorrow \\r\\n©\\r...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[(saving, VBG), (these, DT), (bingo, NN), (ca...</td>\n",
       "      <td>[[save, these, bingo, card, for, tomorrow, ©, ...</td>\n",
       "      <td>[saving, bingo, cards, tomorrow, AvengersEndgame]</td>\n",
       "      <td>[(saving, VBG), (bingo, NN), (cards, NNS), (to...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1         mrvelstan literally nobody AvengersEndgame      False   \n",
       "1           2  agntecarter im emotional sorry 2014 x 2019 bla...      False   \n",
       "2           3        saving bingo cards tomorrow AvengersEndgame      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "1              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "2              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  ... latitude  \\\n",
       "0  1120639328034676737         NaN  ...      NaN   \n",
       "1  1120639325199196160         NaN  ...      NaN   \n",
       "2  1120639324683292674         NaN  ...      NaN   \n",
       "\n",
       "                                    tokens_sentences  \\\n",
       "0  [[RT, @, mrvelstan, :, literally, nobody, :, m...   \n",
       "1  [[RT, @, agntecarter, :, i, ’, m, emotional, ,...   \n",
       "2  [[saving, these, bingo, cards, for, tomorrow, ...   \n",
       "\n",
       "                                           sentences  tweettokens  \\\n",
       "0  [RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r...         None   \n",
       "1  [RT @agntecarter: i’m emotional, sorry!!, 2014...         None   \n",
       "2  [saving these bingo cards for tomorrow \\r\\n©\\r...         None   \n",
       "\n",
       "                 names_avengers  \\\n",
       "0                            []   \n",
       "1  [blackwidow, captainamerica]   \n",
       "2                            []   \n",
       "\n",
       "                                          POS_tokens  \\\n",
       "0  [[(RT, NNP), (@, NNP), (mrvelstan, NN), (:, :)...   \n",
       "1  [[(RT, NNP), (@, NNP), (agntecarter, NN), (:, ...   \n",
       "2  [[(saving, VBG), (these, DT), (bingo, NN), (ca...   \n",
       "\n",
       "                         tokens_sentences_lemmatized  \\\n",
       "0  [[RT, @, mrvelstan, :, literally, nobody, :, m...   \n",
       "1  [[RT, @, agntecarter, :, i, ’, m, emotional, ,...   \n",
       "2  [[save, these, bingo, card, for, tomorrow, ©, ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0    [mrvelstan, literally, nobody, AvengersEndgame]   \n",
       "1  [agntecarter, im, emotional, sorry, 2014, x, 2...   \n",
       "2  [saving, bingo, cards, tomorrow, AvengersEndgame]   \n",
       "\n",
       "                                          pos_tokens       nnp_tokens  \n",
       "0  [(mrvelstan, NNS), (literally, RB), (nobody, V...  AvengersEndgame  \n",
       "1  [(agntecarter, RB), (im, JJ), (emotional, JJ),...                   \n",
       "2  [(saving, VBG), (bingo, NN), (cards, NNS), (to...                   \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "      <th>Bin_High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639325199196160</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NRmalaa</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>saving these bingo cards for tomorrow \\r\\n©\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639324683292674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>jijitsuu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "1           2  RT @agntecarter: i’m emotional, sorry!!\\r\\n\\r\\...      False   \n",
       "2           3  saving these bingo cards for tomorrow \\r\\n©\\r\\...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "1              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "2              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  \\\n",
       "0  1120639328034676737         NaN   \n",
       "1  1120639325199196160         NaN   \n",
       "2  1120639324683292674         NaN   \n",
       "\n",
       "                                        statusSource screenName  retweetCount  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  DavidAc96           637   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...    NRmalaa           302   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...   jijitsuu             0   \n",
       "\n",
       "   isRetweet  retweeted  longitude  latitude  High_Retweet_Count  \\\n",
       "0       True      False        NaN       NaN               False   \n",
       "1       True      False        NaN       NaN               False   \n",
       "2      False      False        NaN       NaN               False   \n",
       "\n",
       "   Bin_High_Retweet_Count  \n",
       "0                       1  \n",
       "1                       1  \n",
       "2                       1  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv('tweets.csv', encoding='cp1252')\n",
    "tweets_df.head(3)\n",
    "\n",
    "tweets_df['retweetCount'] > np.median(tweets_df['retweetCount'])\n",
    "tweets_df['High_Retweet_Count'] = 0\n",
    "\n",
    "tweets_df['High_Retweet_Count'] = tweets_df['retweetCount'] > np.median(tweets_df['retweetCount'])\n",
    "\n",
    "tweets_df['Bin_High_Retweet_Count'] = tweets_df['High_Retweet_Count'].apply(lambda x: 1 if x!= True  else 0)\n",
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "features = ['text', 'favoriteCount', 'screenName']\n",
    "target = ['Bin_High_Retweet_Count']\n",
    "\n",
    "tweets_df['text'] = tweets_df['text'].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df[features], \\\n",
    "                                                    tweets_df[target], \\\n",
    "                                                    test_size=0.34, \\\n",
    "                                                    random_state=2010)\n",
    "\n",
    "pickle.dump(train_test_split(tweets_df[features], \\\n",
    "                                tweets_df[target], \\\n",
    "                                test_size=0.2, \\\n",
    "                                random_state=2020), open('split.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: Add intialization from .yaml descriptions of classifiers to implementations of classifiers at Exercise 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See 5 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11: Train classifiers with various PCA dimensionality, bag of words and polynomial paramters paramteres on train, test them on test and store in .yaml files for every experiment with resulting metrics\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- adjusted_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See 5 and 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Combine all combinations of features from Exercise 8, measure the results of improvement of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Class 0: 150\n",
      "Number of Class 1: 150\n",
      "Ground truth shape is (300,).\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# temporary solution for relative imports in case combo is not installed\n",
    "# if combo is installed, no need to use the following line\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "# supress warnings for clean output\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# Import all models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from combo.models.classifier_comb import SimpleClassifierAggregator\n",
    "from combo.models.classifier_stacking import Stacking\n",
    "from combo.models.classifier_dcs import DCS_LA\n",
    "from combo.models.classifier_des import DES_LA\n",
    "\n",
    "# Define the number of class 0 and class 1\n",
    "n_samples = 300\n",
    "class1_fraction = 0.5\n",
    "clusters_separation = [3]\n",
    "\n",
    "# Compare given detectors under given settings\n",
    "# Initialize the data\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\n",
    "n_class0 = int((1. - class1_fraction) * n_samples)\n",
    "n_class1 = int(class1_fraction * n_samples)\n",
    "ground_truth = np.zeros(n_samples, dtype=int)\n",
    "ground_truth[-n_class1:] = 1\n",
    "\n",
    "# Show the statics of the data\n",
    "print('Number of Class 0: %i' % n_class0)\n",
    "print('Number of Class 1: %i' % n_class1)\n",
    "print('Ground truth shape is {shape}.\\n'.format(shape=ground_truth.shape))\n",
    "print(ground_truth, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_5_1/config.yaml:\n",
      "Using configuration file experiments/exp_7_1/config.yaml:\n",
      "Using configuration file experiments/exp_7_2/config.yaml:\n",
      "Using configuration file experiments/exp_7_3/config.yaml:\n",
      "Using configuration file experiments/exp_7_4/config.yaml:\n",
      "Using configuration file experiments/exp_7_5/config.yaml:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticCustomClassifier(result_folder_path='experiments/exp_5_1/result/',\n",
       "                          transformer=<class 'experiments.exp_5_1.transformer.Transformer_5_1'>,\n",
       "                          yaml_config_file=None),)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load all custom classifiers from file\n",
    "\n",
    "classifier51 = LogisticCustomClassifier('experiments/exp_5_1/config.yaml', Transformer_5_1, 'experiments/exp_5_1/result/'),\n",
    "classifier71 = LogisticCustomClassifier('experiments/exp_7_1/config.yaml', Transformer_7_1, 'experiments/exp_7_1/result/'),\n",
    "classifier72 = LogisticCustomClassifier('experiments/exp_7_2/config.yaml', Transformer_7_2, 'experiments/exp_7_2/result/'),\n",
    "classifier73 = LogisticCustomClassifier('experiments/exp_7_3/config.yaml', Transformer_7_3, 'experiments/exp_7_3/result/'),\n",
    "classifier74 = LogisticCustomClassifier('experiments/exp_7_4/config.yaml', Transformer_7_4, 'experiments/exp_7_4/result/'),\n",
    "classifier75 = LogisticCustomClassifier('experiments/exp_7_5/config.yaml', Transformer_7_5, 'experiments/exp_7_5/result/')\n",
    "\n",
    "classifier1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_5_1/config.yaml:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticCustomClassifier(result_folder_path='experiments/exp_5_1/result/',\n",
       "                         transformer=<class 'experiments.exp_5_1.transformer.Transformer_5_1'>,\n",
       "                         yaml_config_file=None)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticCustomClassifier('experiments/exp_5_1/config.yaml', Transformer_5_1, 'experiments/exp_5_1/result/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17: in collaboration with classmates create the best possible classifier for the data using various technics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tested a technique from internet which should try all possible variations of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-b3f898bf1ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Show all classifiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'set' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "classifiers = [LogisticRegression(), GaussianNB(), SVC(probability=True),\n",
    "               KNeighborsClassifier()]\n",
    "\n",
    "# Define some combination methods to be compared\n",
    "\n",
    "classifiers = {\n",
    "    classifier51,\n",
    "    classifier71,\n",
    "    classifier72,\n",
    "    classifier73,\n",
    "    classifier74,\n",
    "    classifier75\n",
    "}\n",
    "\n",
    "# Show all classifiers\n",
    "for i, clf in enumerate(classifiers.keys()):\n",
    "    print('Model', i + 1, clf)\n",
    "\n",
    "# Fit the models with the generated data and\n",
    "# compare model performances\n",
    "for i, offset in enumerate(clusters_separation):\n",
    "    np.random.seed(42)\n",
    "    # Data generation\n",
    "    X1 = 0.3 * np.random.randn(n_class0 // 2, 2) - offset\n",
    "    X2 = 0.3 * np.random.randn(n_class0 // 2, 2) + offset\n",
    "    X = np.r_[X1, X2]\n",
    "    # Add class 1\n",
    "    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_class1, 2))]\n",
    "\n",
    "    # Fit the model\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        print(i + 1, 'fitting', clf_name)\n",
    "        # fit the data and tag class 1\n",
    "\n",
    "        clf.fit(X, ground_truth)\n",
    "        scores_pred = clf.predict_proba(X)[:, 1] * -1\n",
    "\n",
    "        y_pred = clf.predict(X)\n",
    "        threshold = percentile(scores_pred, 100 * class1_fraction)\n",
    "        n_errors = (y_pred != ground_truth).sum()\n",
    "        # plot the levels lines and the points\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] * -1\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        subplot = plt.subplot(3, 4, i + 1)\n",
    "        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                         cmap=plt.cm.Blues_r)\n",
    "        a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                            linewidths=2, colors='red')\n",
    "        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                         colors='orange')\n",
    "        b = subplot.scatter(X[:-n_class1, 0], X[:-n_class1, 1], c='white',\n",
    "                            s=20, edgecolor='k')\n",
    "        c = subplot.scatter(X[-n_class1:, 0], X[-n_class1:, 1], c='black',\n",
    "                            s=20, edgecolor='k')\n",
    "        subplot.axis('tight')\n",
    "        subplot.legend(\n",
    "            [a.collections[0], b, c],in collaboration\n",
    "            ['learned boundary', 'class 0', 'class 1'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=10),\n",
    "            loc='lower right')\n",
    "        subplot.set_xlabel(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n",
    "        subplot.set_xlim((-7, 7))\n",
    "        subplot.set_ylim((-7, 7))\n",
    "    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "    plt.suptitle(\"Model Combination\")\n",
    "# plt.savefig('compare_selected_classifiers.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_DS2",
   "language": "python",
   "name": "python_for_ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
