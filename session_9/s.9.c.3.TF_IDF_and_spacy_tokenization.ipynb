{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Berend_Gort/session_9\r\n"
     ]
    }
   ],
   "source": [
    "!cd /home/ubuntu/Berend_Gort/session_9\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text as sk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load( open('Train_and_Test_local.pklz',\"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load( open('df.pklz',\"rb\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "    TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "    IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sk_text.CountVectorizer()\n",
    "X_train_feat = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1422x28242 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 217306 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = sk_text.TfidfTransformer()\n",
    "X_train_tfidf = transformer.fit_transform(X_train_feat)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39942818\n",
      "40160124\n"
     ]
    }
   ],
   "source": [
    "arr = X_train_tfidf == X_train_feat\n",
    "arr\n",
    "\n",
    "print(np.sum(arr.toarray()))\n",
    "print(1422*28242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the whole DF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = sk_text.TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: psyrobtw@ubvmsd.cc.buffalo.edu (Robert Weiss)\n",
      "Subject: 18 Apr 93   God's Promise in Philippians 4:9\n",
      "Organization: University at Buffalo\n",
      "Lines: 8\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      "\tThose things,\n",
      "\twhich ye have both learned, and received,\n",
      "\tand heard, and seen in me,\n",
      "\tdo:\n",
      "\tand the God of peace shall be with you.\n",
      "\n",
      "\tPhilippians 4:9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_text = df.text.iloc[0]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vectorizer.transform([example_text])\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "feature_index = tfidf_vector.nonzero()[1]\n",
    "\n",
    "# feature_index\n",
    "\n",
    "# tfidf_vector\n",
    "\n",
    "# tfidf_vector.nonzero()\n",
    "\n",
    "# tfidf_vector[0, 26362] # (26362, 0.042)\n",
    "\n",
    "# type(tfidf_vector)\n",
    "\n",
    "feature_names[26362]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is only for the example text!\n",
      "amount of locations in matrix that have a value = 49 \n",
      "\n",
      "{'subject': 0.02879840104494834, 'lines': 0.02879840104494834, 'from': 0.02879840104494834, 'organization': 0.030043159068796566, 'the': 0.030463274484824606, 'of': 0.03191706176174092, 'you': 0.042150603089860884, 'have': 0.043449511003470685, 'be': 0.043750018784597215, 'with': 0.04797196442895758, 'posting': 0.05023499424678744, 'host': 0.05214690227578756, 'nntp': 0.05255411347434857, 'do': 0.05400251349258381, 'at': 0.05527611971861685, 'university': 0.05655625940428654, 'me': 0.05784115954968193, 'which': 0.06145295883680894, 'in': 0.06587276031383729, 'those': 0.07449601949689157, 'edu': 0.07931609459002176, 'things': 0.08583310134592778, 'news': 0.09494984233334937, 'both': 0.0970077462363637, 'robert': 0.0976549285518166, 'software': 0.09876697408890935, 'seen': 0.1039722272605738, 'apr': 0.10536372502673985, '18': 0.11171870741361259, 'heard': 0.1128055685144305, '93': 0.11393506234108758, '41': 0.12339545420664484, 'shall': 0.1306329079006603, 'vms': 0.13203798661546046, 'peace': 0.13351515147814974, 'promise': 0.1342831543527876, 'and': 0.13473530348219243, 'vax': 0.13937847584351942, 'weiss': 0.1433569846955854, 'god': 0.1439479305563791, 'vnews': 0.14444384579640326, 'received': 0.151999517104401, 'psyrobtw': 0.151999517104401, 'learned': 0.15347668196709024, 'ye': 0.17499526197755813, 'cc': 0.1953098571036332, 'ubvmsd': 0.3644654313431472, 'philippians': 0.4366205721051264, 'buffalo': 0.44024684337812797}\n"
     ]
    }
   ],
   "source": [
    "tfidf_scores = zip(feature_index, [tfidf_vector[0, x] for x in feature_index])\n",
    "\n",
    "# print(list(tfidf_scores))\n",
    "\n",
    "ws = [(feature_names[i], s) for (i, s) in tfidf_scores]  # /sum_counts[0, cnt_features.index(feature_names[i])]\n",
    "a = ws.sort(key=lambda x: x[1])\n",
    "#top_ws = {w: s for w, s in ws[-10:]}\n",
    "\n",
    "\n",
    "top_ws = {w: s for w, s in ws}\n",
    "\n",
    "print('This is only for the example text!')\n",
    "\n",
    "print('amount of locations in matrix that have a value =', len(top_ws), '\\n')\n",
    "\n",
    "print(top_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"textcat\", \"ner\"])\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp.add_pipe(sentencizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Wall PROPN\n",
      "Street Street PROPN\n",
      "has have AUX\n",
      "fallen fall VERB\n",
      "in in ADP\n",
      "love love NOUN\n",
      "with with ADP\n",
      "Microsoft Microsoft PROPN\n",
      "shares share NOUN\n",
      "all all ADV\n",
      "over over ADV\n",
      "again again ADV\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Wall Street has fallen in love with Microsoft shares all over again.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wall', 'Street', 'Microsoft']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_named_entities = [token.lemma_ for token in doc if token.pos_ =='PROPN']\n",
    "only_named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2: Perform simple tokenization with Spacy of files alice_00.txt, ..., alice_08.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = !ls alice*\n",
    "no_files = len(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“And what is the use of a book,” thought Alice, “without pictures or conversations?”',\n",
       " '“How funny it’ll seem to come out among the people that walk with their heads downwards! The antipathies, I think—”',\n",
       " '“Oh, how I wish I could shut up like a telescope! I think I could, if only I knew how to begin.” For, you see, so many out-of-the-way things had happened lately, that Alice had begun to think that very few things indeed were really impossible.',\n",
       " 'It was all very well to say “Drink me,” but the wise little Alice was not going to do that in a hurry. “No, I’ll look first,” she said, “and see whether it’s marked ‘poison’ or not.”',\n",
       " '“But it’s no use now,” thought poor Alice, “to pretend to be two people! Why, there’s hardly enough of me left to make one respectable person!”',\n",
       " '“Curiouser and curiouser!” cried Alice (she was so much surprised, that for the moment she quite forgot how to speak good English).',\n",
       " '“How do you like the Queen?” said the Cat in a low voice.\\n“Not at all,” said Alice: “she’s so extremely—” Just then she noticed that the Queen was close behind her, listening: so she went on “—likely to win, that it’s hardly worth while finishing the game.”',\n",
       " '“Who are you?” said the Caterpillar.\\nThis was not an encouraging opening for a conversation. Alice replied, rather shyly, “I—I hardly know, Sir, just at present—at least I know who I was when I got up this morning, but I think I must have been changed several times since then.”\\n“What do you mean by that?” said the Caterpillar, sternly. “Explain yourself!”\\n“I ca’n’t explain myself, I’m afraid, Sir,” said Alice, “because I am not myself, you see.”',\n",
       " '“Would you tell me, please, which way I ought to go from here?”\\n“That depends a good deal on where you want to get to,” said the Cat.\\n“I don’t much care where—” said Alice.\\n“Then it doesn’t matter which way you go,” said the Cat.\\n“—so long as I get somewhere,” Alice added as an explanation.\\n“Oh, you’re sure to do that,” said the Cat, “if you only walk long enough.”']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_alice = []\n",
    "for file in list_of_files:\n",
    "    myfile = open(file)\n",
    "    string_alice.append(myfile.read())\n",
    "\n",
    "string_alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“ \" PUNCT\n",
      "And and CCONJ\n",
      "what what PRON\n",
      "is be AUX\n",
      "the the DET\n",
      "use use NOUN\n",
      "of of ADP\n",
      "a a DET\n",
      "book book NOUN\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "thought think VERB\n",
      "Alice Alice PROPN\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "without without ADP\n",
      "pictures picture NOUN\n",
      "or or CCONJ\n",
      "conversations conversation NOUN\n",
      "? ? PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "How how ADV\n",
      "funny funny ADJ\n",
      "it -PRON- PRON\n",
      "’ll will VERB\n",
      "seem seem VERB\n",
      "to to PART\n",
      "come come VERB\n",
      "out out ADP\n",
      "among among ADP\n",
      "the the DET\n",
      "people people NOUN\n",
      "that that DET\n",
      "walk walk VERB\n",
      "with with ADP\n",
      "their -PRON- DET\n",
      "heads head NOUN\n",
      "downwards downwards ADV\n",
      "! ! PUNCT\n",
      "The the DET\n",
      "antipathies antipathy NOUN\n",
      ", , PUNCT\n",
      "I -PRON- PRON\n",
      "think think VERB\n",
      "— — PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "Oh oh INTJ\n",
      ", , PUNCT\n",
      "how how ADV\n",
      "I -PRON- PRON\n",
      "wish wish VERB\n",
      "I -PRON- PRON\n",
      "could could VERB\n",
      "shut shut VERB\n",
      "up up ADP\n",
      "like like SCONJ\n",
      "a a DET\n",
      "telescope telescope NOUN\n",
      "! ! PUNCT\n",
      "I -PRON- PRON\n",
      "think think VERB\n",
      "I -PRON- PRON\n",
      "could could VERB\n",
      ", , PUNCT\n",
      "if if SCONJ\n",
      "only only ADV\n",
      "I -PRON- PRON\n",
      "knew know VERB\n",
      "how how ADV\n",
      "to to PART\n",
      "begin begin VERB\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "For for CCONJ\n",
      ", , PUNCT\n",
      "you -PRON- PRON\n",
      "see see VERB\n",
      ", , PUNCT\n",
      "so so ADV\n",
      "many many ADJ\n",
      "out out ADJ\n",
      "- - PUNCT\n",
      "of of ADP\n",
      "- - PUNCT\n",
      "the the DET\n",
      "- - PUNCT\n",
      "way way NOUN\n",
      "things thing NOUN\n",
      "had have AUX\n",
      "happened happen VERB\n",
      "lately lately ADV\n",
      ", , PUNCT\n",
      "that that SCONJ\n",
      "Alice Alice PROPN\n",
      "had have AUX\n",
      "begun begin VERB\n",
      "to to PART\n",
      "think think VERB\n",
      "that that SCONJ\n",
      "very very ADV\n",
      "few few ADJ\n",
      "things thing NOUN\n",
      "indeed indeed ADV\n",
      "were be AUX\n",
      "really really ADV\n",
      "impossible impossible ADJ\n",
      ". . PUNCT\n",
      "It -PRON- PRON\n",
      "was be AUX\n",
      "all all ADV\n",
      "very very ADV\n",
      "well well ADV\n",
      "to to PART\n",
      "say say VERB\n",
      "“ \" PUNCT\n",
      "Drink drink VERB\n",
      "me -PRON- PRON\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "but but CCONJ\n",
      "the the DET\n",
      "wise wise ADJ\n",
      "little little ADJ\n",
      "Alice Alice PROPN\n",
      "was be AUX\n",
      "not not PART\n",
      "going go VERB\n",
      "to to PART\n",
      "do do AUX\n",
      "that that DET\n",
      "in in ADP\n",
      "a a DET\n",
      "hurry hurry NOUN\n",
      ". . PUNCT\n",
      "“ \" PUNCT\n",
      "No no INTJ\n",
      ", , PUNCT\n",
      "I -PRON- PRON\n",
      "’ll will VERB\n",
      "look look VERB\n",
      "first first ADV\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "she -PRON- PRON\n",
      "said say VERB\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "and and CCONJ\n",
      "see see VERB\n",
      "whether whether SCONJ\n",
      "it -PRON- PRON\n",
      "’s ’ VERB\n",
      "marked mark VERB\n",
      "‘ ' PUNCT\n",
      "poison poison NOUN\n",
      "’ ' PUNCT\n",
      "or or CCONJ\n",
      "not not PART\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "But but CCONJ\n",
      "it -PRON- PRON\n",
      "’s ’ VERB\n",
      "no no DET\n",
      "use use NOUN\n",
      "now now ADV\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "thought think VERB\n",
      "poor poor ADJ\n",
      "Alice Alice PROPN\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "to to PART\n",
      "pretend pretend VERB\n",
      "to to PART\n",
      "be be AUX\n",
      "two two NUM\n",
      "people people NOUN\n",
      "! ! PUNCT\n",
      "Why why ADV\n",
      ", , PUNCT\n",
      "there there PRON\n",
      "’s ’ VERB\n",
      "hardly hardly ADV\n",
      "enough enough ADJ\n",
      "of of ADP\n",
      "me -PRON- PRON\n",
      "left leave VERB\n",
      "to to PART\n",
      "make make VERB\n",
      "one one NUM\n",
      "respectable respectable ADJ\n",
      "person person NOUN\n",
      "! ! PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "Curiouser Curiouser PROPN\n",
      "and and CCONJ\n",
      "curiouser curiouser NOUN\n",
      "! ! PUNCT\n",
      "” \" PUNCT\n",
      "cried cry VERB\n",
      "Alice Alice PROPN\n",
      "( ( PUNCT\n",
      "she -PRON- PRON\n",
      "was be AUX\n",
      "so so ADV\n",
      "much much ADV\n",
      "surprised surprised ADJ\n",
      ", , PUNCT\n",
      "that that SCONJ\n",
      "for for ADP\n",
      "the the DET\n",
      "moment moment NOUN\n",
      "she -PRON- PRON\n",
      "quite quite ADV\n",
      "forgot forget VERB\n",
      "how how ADV\n",
      "to to PART\n",
      "speak speak VERB\n",
      "good good ADJ\n",
      "English English PROPN\n",
      ") ) PUNCT\n",
      ". . PUNCT\n",
      "“ \" PUNCT\n",
      "How how ADV\n",
      "do do AUX\n",
      "you -PRON- PRON\n",
      "like like VERB\n",
      "the the DET\n",
      "Queen Queen PROPN\n",
      "? ? PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Cat Cat PROPN\n",
      "in in ADP\n",
      "a a DET\n",
      "low low ADJ\n",
      "voice voice NOUN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "Not not PART\n",
      "at at ADV\n",
      "all all ADV\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "Alice Alice PROPN\n",
      ": : PUNCT\n",
      "“ \" PUNCT\n",
      "she -PRON- PRON\n",
      "’s ’ VERB\n",
      "so so ADV\n",
      "extremely extremely ADV\n",
      "— — PUNCT\n",
      "” \" PUNCT\n",
      "Just just ADV\n",
      "then then ADV\n",
      "she -PRON- PRON\n",
      "noticed notice VERB\n",
      "that that SCONJ\n",
      "the the DET\n",
      "Queen Queen PROPN\n",
      "was be AUX\n",
      "close close ADJ\n",
      "behind behind ADP\n",
      "her -PRON- PRON\n",
      ", , PUNCT\n",
      "listening listen VERB\n",
      ": : PUNCT\n",
      "so so ADV\n",
      "she -PRON- PRON\n",
      "went go VERB\n",
      "on on ADP\n",
      "“ \" PUNCT\n",
      "— — PUNCT\n",
      "likely likely ADJ\n",
      "to to PART\n",
      "win win VERB\n",
      ", , PUNCT\n",
      "that that SCONJ\n",
      "it -PRON- PRON\n",
      "’s ’ VERB\n",
      "hardly hardly ADV\n",
      "worth worth ADJ\n",
      "while while SCONJ\n",
      "finishing finish VERB\n",
      "the the DET\n",
      "game game NOUN\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "Who who PRON\n",
      "are be AUX\n",
      "you -PRON- PRON\n",
      "? ? PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Caterpillar Caterpillar PROPN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "This this DET\n",
      "was be AUX\n",
      "not not PART\n",
      "an an DET\n",
      "encouraging encouraging ADJ\n",
      "opening opening NOUN\n",
      "for for ADP\n",
      "a a DET\n",
      "conversation conversation NOUN\n",
      ". . PUNCT\n",
      "Alice Alice PROPN\n",
      "replied reply VERB\n",
      ", , PUNCT\n",
      "rather rather ADV\n",
      "shyly shyly ADV\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "I -PRON- PRON\n",
      "— — PUNCT\n",
      "I -PRON- PRON\n",
      "hardly hardly ADV\n",
      "know know VERB\n",
      ", , PUNCT\n",
      "Sir Sir PROPN\n",
      ", , PUNCT\n",
      "just just ADV\n",
      "at at ADP\n",
      "present present NOUN\n",
      "— — PUNCT\n",
      "at at ADV\n",
      "least least ADJ\n",
      "I -PRON- PRON\n",
      "know know VERB\n",
      "who who PRON\n",
      "I -PRON- PRON\n",
      "was be AUX\n",
      "when when ADV\n",
      "I -PRON- PRON\n",
      "got get VERB\n",
      "up up ADP\n",
      "this this DET\n",
      "morning morning NOUN\n",
      ", , PUNCT\n",
      "but but CCONJ\n",
      "I -PRON- PRON\n",
      "think think VERB\n",
      "I -PRON- PRON\n",
      "must must VERB\n",
      "have have AUX\n",
      "been be AUX\n",
      "changed change VERB\n",
      "several several ADJ\n",
      "times time NOUN\n",
      "since since SCONJ\n",
      "then then ADV\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "What what PRON\n",
      "do do AUX\n",
      "you -PRON- PRON\n",
      "mean mean VERB\n",
      "by by ADP\n",
      "that that DET\n",
      "? ? PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Caterpillar Caterpillar PROPN\n",
      ", , PUNCT\n",
      "sternly sternly ADV\n",
      ". . PUNCT\n",
      "“ \" PUNCT\n",
      "Explain explain VERB\n",
      "yourself -PRON- PRON\n",
      "! ! PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "I -PRON- PRON\n",
      "ca’n’t ca’n’t VERB\n",
      "explain explain VERB\n",
      "myself -PRON- PRON\n",
      ", , PUNCT\n",
      "I -PRON- PRON\n",
      "’m be VERB\n",
      "afraid afraid ADJ\n",
      ", , PUNCT\n",
      "Sir Sir PROPN\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "Alice Alice PROPN\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "because because SCONJ\n",
      "I -PRON- PRON\n",
      "am be AUX\n",
      "not not PART\n",
      "myself -PRON- PRON\n",
      ", , PUNCT\n",
      "you -PRON- PRON\n",
      "see see VERB\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "“ \" PUNCT\n",
      "Would Would VERB\n",
      "you -PRON- PRON\n",
      "tell tell VERB\n",
      "me -PRON- PRON\n",
      ", , PUNCT\n",
      "please please INTJ\n",
      ", , PUNCT\n",
      "which which DET\n",
      "way way NOUN\n",
      "I -PRON- PRON\n",
      "ought ought VERB\n",
      "to to PART\n",
      "go go VERB\n",
      "from from ADP\n",
      "here here ADV\n",
      "? ? PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "That that DET\n",
      "depends depend VERB\n",
      "a a DET\n",
      "good good ADJ\n",
      "deal deal NOUN\n",
      "on on ADP\n",
      "where where ADV\n",
      "you -PRON- PRON\n",
      "want want VERB\n",
      "to to PART\n",
      "get get AUX\n",
      "to to ADP\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Cat Cat PROPN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "I -PRON- PRON\n",
      "do do AUX\n",
      "n’t not PART\n",
      "much much ADJ\n",
      "care care VERB\n",
      "where where ADV\n",
      "— — PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "Alice Alice PROPN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "Then then ADV\n",
      "it -PRON- PRON\n",
      "does do AUX\n",
      "n’t not PART\n",
      "matter matter VERB\n",
      "which which DET\n",
      "way way NOUN\n",
      "you -PRON- PRON\n",
      "go go VERB\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Cat Cat PROPN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "— — PUNCT\n",
      "so so ADV\n",
      "long long ADV\n",
      "as as SCONJ\n",
      "I -PRON- PRON\n",
      "get get VERB\n",
      "somewhere somewhere ADV\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "Alice Alice PROPN\n",
      "added add VERB\n",
      "as as SCONJ\n",
      "an an DET\n",
      "explanation explanation NOUN\n",
      ". . PUNCT\n",
      "\n",
      " \n",
      " SPACE\n",
      "“ \" PUNCT\n",
      "Oh oh INTJ\n",
      ", , PUNCT\n",
      "you -PRON- PRON\n",
      "’re be VERB\n",
      "sure sure ADJ\n",
      "to to PART\n",
      "do do AUX\n",
      "that that DET\n",
      ", , PUNCT\n",
      "” \" PUNCT\n",
      "said say VERB\n",
      "the the DET\n",
      "Cat Cat PROPN\n",
      ", , PUNCT\n",
      "“ \" PUNCT\n",
      "if if SCONJ\n",
      "you -PRON- PRON\n",
      "only only ADV\n",
      "walk walk VERB\n",
      "long long ADV\n",
      "enough enough ADV\n",
      ". . PUNCT\n",
      "” \" PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AndandCCONJ'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_per_doc = []\n",
    "# {1: {token: 's', lemma: 'x', pos: 'VERB'}}\n",
    "# {'setence in the alice document': 's x VERB'}\n",
    "for string_i in string_alice:\n",
    "    doc = nlp(string_i)\n",
    "    for token in doc:\n",
    "        tokens_per_doc.append(token.text + token.lemma_ + token.pos_)\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "        \n",
    "tokens_per_doc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
