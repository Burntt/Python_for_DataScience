{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Download the dataset\n",
    "\n",
    "https://www.kaggle.com/kavita5/twitter-dataset-avengersendgame/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: \n",
    "Create Target column from retweetCount > np.median[retweetCount] \n",
    "\n",
    "Create dataset in .csv file with new features along with .yaml file with it's descritpion, median[retweetCount] value stored in retweetCount_median variable, preferably store data in data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweets_df' (DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  \\\n",
       "0  1120639328034676737         NaN   \n",
       "\n",
       "                                        statusSource screenName  retweetCount  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...  DavidAc96           637   \n",
       "\n",
       "   isRetweet  retweeted  longitude  latitude  High_Retweet_Count  \n",
       "0       True      False        NaN       NaN                   0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add column to dataframe\n",
    "tweets_df = pd.read_csv('./data/tweets.csv', encoding='cp1252')\n",
    "tweets_df['High_Retweet_Count'] = tweets_df['retweetCount'] > np.median(tweets_df['retweetCount'])\n",
    "tweets_df['High_Retweet_Count'].replace({False: 0, True: 1}, inplace=True)\n",
    "\n",
    "%store tweets_df\n",
    "tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/new_tweets: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# Save new csv\n",
    "!mkdir data/new_tweets\n",
    "tweets_df.to_csv('./data/new_tweets/new_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/new_tweets/new_tweets.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile './data/new_tweets/new_tweets.yaml'\n",
    "\n",
    "# new_tweets.csv\n",
    "\n",
    "description: This dataset is the same as ./data/tweets.csv with the addition of High_Retweet_Count column.\n",
    "    Also median[retweetCount] value is stored in retweetCount_median variable.\n",
    "    \n",
    "retweetCount_median: 1755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Collaborate at github account\n",
    "\n",
    "https://github.com/maxmmsu/harbour.space.DS.402\n",
    "\n",
    "Create an .md file in it with the table of participants (one for a group), select a short acronim for your name (e.g. Maxim Musin --> mm) and write down the acronim to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created table row with my name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Ð¡reate a folder with your first exeperimet, name it exp_[your name acronim]_1, make it a python package, organize all transformers and classifiers wih python modules and .yaml files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create them later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5: Select one of the following feature creation technics, write it down to collaboration table\n",
    "\n",
    "##### Step 1 create 3 of the following features\n",
    "- Extract lenght of the sentence\n",
    "- Extract number of words\n",
    "- Extract number of characters\n",
    "- Extract number of hastags\n",
    "\n",
    "- Create a dummy variable for RT at the beginning of the twit\n",
    "- Creatu dummy variable from first letter of twit being A, B, C or D\n",
    "\n",
    "##### Step 2 preform polynomial feature generation with parameter k = 2..6 \n",
    "\n",
    "##### Step 3 perform dimensionality reduction to limit number of vatiables\n",
    "- perform PCA dimensionality reduction\n",
    "- *prefromr PCA dimensionality reduction according to covered variance\n",
    "- select features with maximum mutual information with target variable\n",
    "- *select features according to sum of adjusted mutual info score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: \n",
    "#### Extract number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_words(sentence):\n",
    "    sentence_array = word_tokenize(sentence)\n",
    "    words = [word for word in sentence_array if word.isalpha()]\n",
    "    return len(words)\n",
    "\n",
    "tweets_df['number_of_words'] = tweets_df['text'].apply(number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_chars(sentence):\n",
    "    return len(sentence)\n",
    "\n",
    "tweets_df['number_of_chars'] = tweets_df['text'].apply(number_of_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract number of hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_hashtags(sentence):\n",
    "    return len(re.findall('#\\w+', sentence))\n",
    "\n",
    "tweets_df['number_of_hashtags'] = tweets_df['text'].apply(number_of_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweets_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "tweets_df.head(1)\n",
    "%store tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "#### Polynomial features, degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'favoriteCount',\n",
       " 'replyToSID',\n",
       " 'id',\n",
       " 'replyToUID',\n",
       " 'retweetCount',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'High_Retweet_Count',\n",
       " 'number_of_words',\n",
       " 'number_of_chars',\n",
       " 'number_of_hashtags']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check numerical features\n",
    "list(tweets_df.select_dtypes(include=[np.number]).columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Unnamed: 0                0\n",
       " text                      0\n",
       " favorited                 0\n",
       " favoriteCount             0\n",
       " replyToSN             14603\n",
       " created                   0\n",
       " truncated                 0\n",
       " replyToSID            14631\n",
       " id                        0\n",
       " replyToUID            14603\n",
       " statusSource              0\n",
       " screenName                0\n",
       " retweetCount              0\n",
       " isRetweet                 0\n",
       " retweeted                 0\n",
       " longitude             14996\n",
       " latitude              14996\n",
       " High_Retweet_Count        0\n",
       " number_of_words           0\n",
       " number_of_chars           0\n",
       " number_of_hashtags        0\n",
       " dtype: int64,\n",
       " (15000, 21))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many are null\n",
    "tweets_df.isnull().sum(), tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 35), numpy.ndarray)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features = [\n",
    " 'favoriteCount',\n",
    "#  'retweetCount', not used because is heavly corelated with the target, High_Retweet_Count\n",
    " 'number_of_words',\n",
    " 'number_of_chars',\n",
    " 'number_of_hashtags']\n",
    "\n",
    "poly = preprocessing.PolynomialFeatures(degree=3)\n",
    "\n",
    "transformed_df = poly.fit_transform(tweets_df[numerical_features])\n",
    "\n",
    "transformed_df.shape, type(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:\n",
    "#### Perform PCA reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "transformed_df = scaler.fit_transform(transformed_df)\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "transformed_df = pca.fit_transform(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA reduction based on variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA0</th>\n",
       "      <th>PCA1</th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.559881</td>\n",
       "      <td>-3.757546</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>-0.020212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.218621</td>\n",
       "      <td>-0.998774</td>\n",
       "      <td>1.606653</td>\n",
       "      <td>0.036647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.533296</td>\n",
       "      <td>-3.506074</td>\n",
       "      <td>0.830690</td>\n",
       "      <td>-0.022568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.515959</td>\n",
       "      <td>-3.335006</td>\n",
       "      <td>0.714966</td>\n",
       "      <td>-0.027196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.306377</td>\n",
       "      <td>-1.773567</td>\n",
       "      <td>1.830159</td>\n",
       "      <td>0.029261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-0.374711</td>\n",
       "      <td>-1.680291</td>\n",
       "      <td>-2.169746</td>\n",
       "      <td>-0.038321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.267378</td>\n",
       "      <td>4.330253</td>\n",
       "      <td>-4.685741</td>\n",
       "      <td>-0.167869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.185075</td>\n",
       "      <td>2.669166</td>\n",
       "      <td>0.194137</td>\n",
       "      <td>0.034035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>-0.306377</td>\n",
       "      <td>-1.773567</td>\n",
       "      <td>1.830159</td>\n",
       "      <td>0.029261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>-0.249501</td>\n",
       "      <td>-1.265074</td>\n",
       "      <td>1.670865</td>\n",
       "      <td>0.030785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PCA0      PCA1      PCA2      PCA3\n",
       "0     -0.559881 -3.757546  0.974194 -0.020212\n",
       "1     -0.218621 -0.998774  1.606653  0.036647\n",
       "2     -0.533296 -3.506074  0.830690 -0.022568\n",
       "3     -0.515959 -3.335006  0.714966 -0.027196\n",
       "4     -0.306377 -1.773567  1.830159  0.029261\n",
       "...         ...       ...       ...       ...\n",
       "14995 -0.374711 -1.680291 -2.169746 -0.038321\n",
       "14996  0.267378  4.330253 -4.685741 -0.167869\n",
       "14997  0.185075  2.669166  0.194137  0.034035\n",
       "14998 -0.306377 -1.773567  1.830159  0.029261\n",
       "14999 -0.249501 -1.265074  1.670865  0.030785\n",
       "\n",
       "[15000 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to keep components that have at least .9 variance\n",
    "ratios_sum = 0\n",
    "ratios_index = 0\n",
    "ratios = list(pca.explained_variance_ratio_)\n",
    "\n",
    "while np.sum(ratios[:ratios_index]) < 0.9 and ratios_index < len(ratios):\n",
    "    ratios_index += 1\n",
    "        \n",
    "# So we choose only the first 4 features\n",
    "pca = decomposition.PCA(n_components=ratios_index)\n",
    "# transformed_df = pca.fit_transform(transformed_df)\n",
    "\n",
    "transformed_df = pd.DataFrame(pca.fit_transform(transformed_df), columns=['PCA%i' % i for i in range(4)])\n",
    "\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select features with maximum mutual information with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA2</th>\n",
       "      <th>PCA3</th>\n",
       "      <th>PCA0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.974194</td>\n",
       "      <td>-0.020212</td>\n",
       "      <td>-0.559881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.606653</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>-0.218621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.830690</td>\n",
       "      <td>-0.022568</td>\n",
       "      <td>-0.533296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.714966</td>\n",
       "      <td>-0.027196</td>\n",
       "      <td>-0.515959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.830159</td>\n",
       "      <td>0.029261</td>\n",
       "      <td>-0.306377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-2.169746</td>\n",
       "      <td>-0.038321</td>\n",
       "      <td>-0.374711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>-4.685741</td>\n",
       "      <td>-0.167869</td>\n",
       "      <td>0.267378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.194137</td>\n",
       "      <td>0.034035</td>\n",
       "      <td>0.185075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>1.830159</td>\n",
       "      <td>0.029261</td>\n",
       "      <td>-0.306377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>1.670865</td>\n",
       "      <td>0.030785</td>\n",
       "      <td>-0.249501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PCA2      PCA3      PCA0\n",
       "0      0.974194 -0.020212 -0.559881\n",
       "1      1.606653  0.036647 -0.218621\n",
       "2      0.830690 -0.022568 -0.533296\n",
       "3      0.714966 -0.027196 -0.515959\n",
       "4      1.830159  0.029261 -0.306377\n",
       "...         ...       ...       ...\n",
       "14995 -2.169746 -0.038321 -0.374711\n",
       "14996 -4.685741 -0.167869  0.267378\n",
       "14997  0.194137  0.034035  0.185075\n",
       "14998  1.830159  0.029261 -0.306377\n",
       "14999  1.670865  0.030785 -0.249501\n",
       "\n",
       "[15000 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = feature_selection.mutual_info_classif(transformed_df, tweets_df['High_Retweet_Count'])\n",
    "feature_scores = {feature: scores[feature] for feature in range(transformed_df.shape[1])}\n",
    "feature_scores = ['PCA' + str(k) for k, v in sorted(feature_scores.items(), key=lambda item: item[1], reverse=True)]\n",
    "best_features = feature_scores[0 : int(len(feature_scores) * .75)]\n",
    "\n",
    "transformed_df[best_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIME ðµð»ââï¸\n",
    "##### Experiment 5_1\n",
    "\n",
    "This experiment takes numerical features, generates new features using PolynomialFeatures, reduces features with PCA and classifies the data using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_5_1: File exists\n",
      "mkdir: experiments/exp_5_1/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_5_1\n",
    "!mkdir experiments/exp_5_1/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/config.yaml\n",
    "\n",
    "description: This experiment takes numerical features, generates new features using PolynomialFeatures, reduces features with PCA and classifies the data using Logistic Regression.\n",
    "    \n",
    "features: favoriteCount, number_of_words, number_of_chars, number_of_hashtags\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - target\n",
    "    - pca_components\n",
    "    - poly_degree\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None\n",
    "poly_degree: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_5_1(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('punkt')\n",
    "        return None\n",
    "    \n",
    "    def number_of_words(self, sentence):\n",
    "        sentence_array = word_tokenize(sentence)\n",
    "        words = [word for word in sentence_array if word.isalpha()]\n",
    "        return len(words)\n",
    "    \n",
    "    def number_of_chars(self, sentence):\n",
    "        return len(sentence)\n",
    "\n",
    "    def number_of_hashtags(self, sentence):\n",
    "        return len(re.findall('#\\w+', sentence))\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        X['number_of_words'] = X['text'].apply(self.number_of_words)\n",
    "        X['number_of_chars'] = X['text'].apply(self.number_of_chars)\n",
    "        X['number_of_hashtags'] = X['text'].apply(self.number_of_hashtags)\n",
    "\n",
    "        poly = preprocessing.PolynomialFeatures(degree=3)\n",
    "        transformed_df = poly.fit_transform(X.drop('text', axis=1))\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_5_1/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_5_1/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_5_1.transformer import Transformer_5_1\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = ['text', 'favoriteCount']\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_5_1/config.yaml', Transformer_5_1, 'experiments/exp_5_1/result/')\n",
    "classifier.fit(X_train[features], y_train[target])\n",
    "y_pred = classifier.predict(X_test[features])\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_5_1/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 34, 'accuracy': 0.6666666666666666, 'precision': 0.7267796610169491, 'recall': 0.6423007789095266, 'mutual_info_score': 0.08410396469187705}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_5_1/result/result_02-Nov-2020-19-58-05.yaml\n",
      "CPU times: user 4.34 s, sys: 27.3 ms, total: 4.37 s\n",
      "Wall time: 3.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_5_1/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: Extract names of avengers endgame characters from text \n",
    "\n",
    "Examples of tags in quetion:\n",
    "#Thanos \n",
    "#WinterSoldier \n",
    "#CaptainAmerica \n",
    "#blackwidow\n",
    "#CaptainMarvel\n",
    "#Mantis \n",
    "\n",
    "store the resulting dataset in data folder with .yaml description, preferable naming for variable names is lowercase name of the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of heroes: 43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackwidow</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>captainamerica</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wintersoldier</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanos</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thor</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Hashtag  Count\n",
       "1      blackwidow    567\n",
       "2  captainamerica   1019\n",
       "3   wintersoldier    160\n",
       "4          thanos    176\n",
       "5            thor    159"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r tweets_df\n",
    "\n",
    "import re\n",
    "\n",
    "all_text = \" \".join(list(tweets_df['text']))\n",
    "\n",
    "tags_dict = {}\n",
    "heroes = [\"Iron Man\", \"Thor\", \"Captain America\", \"Captain Marvel\", \"Black Widow\", \"Hawkeye\", \"Hulk\", \"Vision\", \"Scarlet Witch\", \"War Machine\", \"Falcon\", \"StarLord\", \"Rocket Raccoon\", \"Groot\", \"Gamora\", \"Drax\", \"Mantis\", \"Nebula\", \"Doctor Strange\", \"Wong\", \"Spider-Man\", \"Spiderman\", \"Winter Soldier\", \"Heimdall\", \"Black Panther\", \"Okoye\", \"Shuri\", \"MâBaku\", \"Eitiri\", \"Nick Fury\", \"Maria Hill\", \"Pepper Potts\", \"William âThunderboltâ Ross\", \"Ned\", \"Thanos\", \"Loki\", \"the Collector\", \"Cull Obsidian\", \"Ebony Maw\", \"Proxima Midnight\", \"Corvus Glaive\", \"Red Skull\", \"The Wasp\"]\n",
    "heroes = [h.lower().replace(\" \", \"\") for h in heroes]\n",
    "\n",
    "print(\"Number of heroes: \" + str(len(heroes)))\n",
    "\n",
    "# gather all hashtags\n",
    "for tag in re.findall('#\\w+', all_text):\n",
    "    tag = tag[1:].lower().replace(\" \", \"\")\n",
    "    if tag in tags_dict:\n",
    "        tags_dict[tag] += 1\n",
    "    else:\n",
    "        tags_dict[tag] = 1\n",
    "        \n",
    "# remove the non-heroes hashtags\n",
    "for tag in list(tags_dict):\n",
    "    if tag not in heroes:\n",
    "        del tags_dict[tag]\n",
    "        \n",
    "# change the dict for usage in dataframe\n",
    "index = 0\n",
    "for tag in list(tags_dict):\n",
    "    index += 1\n",
    "    tags_dict[index] = [tag, tags_dict.pop(tag)]\n",
    "        \n",
    "# create dataframe from dictionary of heroes\n",
    "tags_df = pd.DataFrame.from_dict(tags_dict, orient='index', columns=['Hashtag', 'Count'])\n",
    "\n",
    "tags_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./data/hashtags: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data/hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tags_df, open('./data/hashtags/hashtags.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/hashtags/hashtags.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile './data/hashtags/hashtags.yaml'\n",
    "\n",
    "description: This dataset consists of hashtags that contain the names of the characters in Avengers Endgame movie\n",
    "    \n",
    "columns: \n",
    "    - Hashtag\n",
    "    - Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: Create set of classifiers for logistic regression with different features for feature generation. \n",
    "use the following methods of feature generation\n",
    "\n",
    " - word tokenization with 1-gram, f\n",
    " - pos tokenization with extraction of all NP \n",
    " - pos tokenization with 2-gram bag of tokens for all NNP\n",
    " - 2-gram bag of characters for screenName features\n",
    " - polynomial features for presence of avenger character names \n",
    " \n",
    "Organize classes of transformers in various .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweets_txt_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# init txt_df\n",
    "tweets_txt_df = tweets_df[['text']]\n",
    "tweets_txt_df.head()\n",
    "\n",
    "%store tweets_txt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ['RT']\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore'))\n",
    "    words = re.sub(r'[^\\w\\s]', '', text) \n",
    "    words = re.sub(r'(http.+)', '', words).split()\n",
    "    return ' '.join([word for word in words if word not in stopwords])\n",
    "#     return ' '.join([wnl.lemmatize(word) for word in words if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrvelstan literally nobody AvengersEndgame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agntecarter im emotional sorry 2014 x 2019 bla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0         mrvelstan literally nobody AvengersEndgame\n",
       "1  agntecarter im emotional sorry 2014 x 2019 bla..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_txt_df['text'] = tweets_txt_df['text'].apply(clean_text)\n",
    "tweets_txt_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 5888), (15000, 1), ['aftershocks', 'afterwards'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_txt_df = vectorizer.fit_transform(tweets_txt_df['text'])\n",
    "\n",
    "vectorized_txt_df.shape, tweets_txt_df.shape, vectorizer.get_feature_names()[300:302]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIMEð¨ð»âð¬\n",
    "##### Experiment 7_1\n",
    "\n",
    "This experiment uses 1-gram tokenization and logistic regression for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_7_1: File exists\n",
      "mkdir: experiments/exp_7_1/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_1\n",
    "!mkdir experiments/exp_7_1/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/config.yaml\n",
    "\n",
    "description: This experiment uses 1-gram tokenization and logistic regression for classification. \n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance greater than .9\n",
    "    \n",
    "features: 1-gram tokenization of text\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - target\n",
    "    - pca_components\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/transformer.py\n",
    "### Custom Transformer for 1-gram tokenization\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_1(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = X.apply(self.clean_text)\n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X)\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_1/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_1/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_1.transformer import Transformer_7_1\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_1/config.yaml', Transformer_7_1, 'experiments/exp_7_1/result/')\n",
    "classifier.fit(X_train[features], y_train[target])\n",
    "y_pred = classifier.predict(X_test[features])\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_1/config.yaml:\n",
      "------------------\n",
      "{'pca_components': 633, 'accuracy': 0.5606666666666666, 'precision': 0.5030508474576271, 'recall': 0.5591559909570459, 'mutual_info_score': 0.01026291882307718}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_1/result/result_02-Nov-2020-20-35-14.yaml\n",
      "CPU times: user 6min 18s, sys: 4.56 s, total: 6min 23s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_1/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tokenization with extraction of all NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrvelstan literally nobody AvengersEndgame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>agntecarter im emotional sorry 2014 x 2019 bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saving bingo cards tomorrow AvengersEndgame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HelloBoon Man AvengersEndgame ads everywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marvel We salute ChrisEvans CaptainAmerica Ave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0         mrvelstan literally nobody AvengersEndgame\n",
       "1  agntecarter im emotional sorry 2014 x 2019 bla...\n",
       "2        saving bingo cards tomorrow AvengersEndgame\n",
       "3       HelloBoon Man AvengersEndgame ads everywhere\n",
       "4  Marvel We salute ChrisEvans CaptainAmerica Ave..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_txt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     \n",
       "1    sorry x blackwidow captainamerica\n",
       "2                       bingo tomorrow\n",
       "Name: nn_tokens, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# tokenize words\n",
    "tweets_txt_df['tokens'] = tweets_txt_df['text'].apply(word_tokenize)\n",
    "\n",
    "# pos tokenize words\n",
    "tweets_txt_df['pos_tokens'] = tweets_txt_df['tokens'].apply(pos_tag)\n",
    "\n",
    "# extract nn\n",
    "tweets_txt_df['nn_tokens'] = tweets_txt_df['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NN'])\n",
    "\n",
    "# make nn into single strings\n",
    "tweets_txt_df['nn_tokens'] = tweets_txt_df['nn_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# show nn strings\n",
    "tweets_txt_df['nn_tokens'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 1700), (15000, 4))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_nn_df = vectorizer.fit_transform(tweets_txt_df['nn_tokens'])\n",
    "\n",
    "vectorized_nn_df.shape, tweets_txt_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIMEð¨ð»âð¬\n",
    "##### Experiment 7_2\n",
    "\n",
    "This experiment uses pos tokenization with extraction of NN and logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_7_2: File exists\n",
      "mkdir: experiments/exp_7_2/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_2\n",
    "!mkdir experiments/exp_7_2/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_2/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/__init__.py\n",
    "\n",
    "# just an empty python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_2/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/config.yaml\n",
    "\n",
    "description: This experiment uses pos tokenization with extraction of NN and logistic regression for classification. \n",
    "    Choosing None for pca_components will automate the process by selecting the components with sum variance greater than .9\n",
    "    \n",
    "features: pos tokenization with extraction of NN\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_2/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_2(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('tagsets')\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):        \n",
    "        X['text'] = X['text'].apply(self.clean_text)\n",
    "        \n",
    "        # tokenize words\n",
    "        X['tokens'] = X['text'].apply(word_tokenize)\n",
    "\n",
    "        # pos tokenize words\n",
    "        X['pos_tokens'] = X['tokens'].apply(pos_tag)\n",
    "\n",
    "        # extract nn\n",
    "        X['nn_tokens'] = X['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NN'])\n",
    "\n",
    "        # make nn into single strings\n",
    "        X['nn_tokens'] = X['nn_tokens'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        vectorizer = CountVectorizer()\n",
    "        transformer_df = vectorizer.fit_transform(X['nn_tokens'])\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_2/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_2/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_2.transformer import Transformer_7_2\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_2/config.yaml', Transformer_7_2, 'experiments/exp_7_2/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_2/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 523, 'accuracy': 0.6406666666666667, 'precision': 0.8074576271186441, 'recall': 0.6, 'mutual_info_score': 0.07050774596867938}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_2/result/result_02-Nov-2020-21-05-18.yaml\n",
      "CPU times: user 34 s, sys: 1.1 s, total: 35.1 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_2/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tokenization with 2-gram bag of tokens for all NNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          AvengersEndgame\n",
       "1                                         \n",
       "2                                         \n",
       "3            HelloBoon Man AvengersEndgame\n",
       "4    Marvel CaptainAmerica AvengersEndgame\n",
       "Name: nnp_tokens, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# tokenize words\n",
    "tweets_txt_df['tokens'] = tweets_txt_df['text'].apply(word_tokenize)\n",
    "\n",
    "# pos tokenize words\n",
    "tweets_txt_df['pos_tokens'] = tweets_txt_df['tokens'].apply(pos_tag)\n",
    "\n",
    "# extract nnp\n",
    "tweets_txt_df['nnp_tokens'] = tweets_txt_df['pos_tokens'].apply(lambda token_list: [token[0] for token in token_list if token[1] == 'NNP'])\n",
    "\n",
    "# make nnnp into single strings\n",
    "tweets_txt_df['nnp_tokens'] = tweets_txt_df['nnp_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# show nnbp strings\n",
    "tweets_txt_df['nnp_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 5471), ['alienware m15', 'alilchubbypod thursday'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "vectorized_nnp_df = vectorizer.fit_transform(tweets_txt_df['nnp_tokens'])\n",
    "\n",
    "vectorized_nnp_df.shape, vectorizer.get_feature_names()[100:102]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIMEð¨ð»âð¬\n",
    "##### Experiment 7_3\n",
    "\n",
    "This experiment uses pos tokenization with extraction of NNP (2-gram) and logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_7_3: File exists\n",
      "mkdir: experiments/exp_7_3/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_3\n",
    "!mkdir experiments/exp_7_3/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_3/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_3/config.yaml\n",
    "\n",
    "description: This experiment uses pos tokenization with extraction of NNP (2-gram) and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: pos tokenization with extraction of NN\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l1\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_3/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_3/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_3.transformer import Transformer_7_3\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_3/config.yaml', Transformer_7_3, 'experiments/exp_7_3/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_3/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "/opt/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 721, 'accuracy': 0.6903333333333334, 'precision': 0.7294915254237289, 'recall': 0.6699875466998755, 'mutual_info_score': 0.1086167869256012}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_3/result/result_02-Nov-2020-21-27-57.yaml\n",
      "CPU times: user 1min 24s, sys: 1.78 s, total: 1min 26s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_3/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram bag of characters for screenName features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              DavidAc96\n",
       "1                NRmalaa\n",
       "2               jijitsuu\n",
       "3               SahapunB\n",
       "4            stella22_97\n",
       "              ...       \n",
       "14995          tommysboi\n",
       "14996    kimberleywithae\n",
       "14997         Gnanavel07\n",
       "14998         _moonljght\n",
       "14999    CaterinaCabrel1\n",
       "Name: screenName, Length: 15000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['screenName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 1271),\n",
       " ['00',\n",
       "  '01',\n",
       "  '02',\n",
       "  '03',\n",
       "  '04',\n",
       "  '05',\n",
       "  '06',\n",
       "  '07',\n",
       "  '08',\n",
       "  '09',\n",
       "  '0_',\n",
       "  '0a',\n",
       "  '0b',\n",
       "  '0c',\n",
       "  '0d',\n",
       "  '0f',\n",
       "  '0h',\n",
       "  '0i',\n",
       "  '0k',\n",
       "  '0l',\n",
       "  '0m',\n",
       "  '0n',\n",
       "  '0o',\n",
       "  '0p',\n",
       "  '0r',\n",
       "  '0s',\n",
       "  '0t',\n",
       "  '0u',\n",
       "  '0v',\n",
       "  '0w',\n",
       "  '0x',\n",
       "  '0y',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '1_',\n",
       "  '1a',\n",
       "  '1b',\n",
       "  '1c',\n",
       "  '1d',\n",
       "  '1e',\n",
       "  '1f',\n",
       "  '1h',\n",
       "  '1j',\n",
       "  '1k',\n",
       "  '1l',\n",
       "  '1m',\n",
       "  '1n',\n",
       "  '1p',\n",
       "  '1q',\n",
       "  '1r',\n",
       "  '1s',\n",
       "  '1t',\n",
       "  '1v',\n",
       "  '20',\n",
       "  '21',\n",
       "  '22',\n",
       "  '23',\n",
       "  '24',\n",
       "  '25',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '2_',\n",
       "  '2a',\n",
       "  '2b',\n",
       "  '2c',\n",
       "  '2d',\n",
       "  '2e',\n",
       "  '2f',\n",
       "  '2g',\n",
       "  '2i',\n",
       "  '2j',\n",
       "  '2k',\n",
       "  '2m',\n",
       "  '2n',\n",
       "  '2p',\n",
       "  '2q',\n",
       "  '2r',\n",
       "  '2s',\n",
       "  '2u',\n",
       "  '2v',\n",
       "  '2w',\n",
       "  '2y',\n",
       "  '2z',\n",
       "  '30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '3_',\n",
       "  '3a',\n",
       "  '3b',\n",
       "  '3c',\n",
       "  '3d',\n",
       "  '3e',\n",
       "  '3f',\n",
       "  '3h',\n",
       "  '3k',\n",
       "  '3l',\n",
       "  '3n',\n",
       "  '3p',\n",
       "  '3q',\n",
       "  '3r',\n",
       "  '3s',\n",
       "  '3t',\n",
       "  '3u',\n",
       "  '3v',\n",
       "  '3w',\n",
       "  '3y',\n",
       "  '3z',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '4_',\n",
       "  '4a',\n",
       "  '4c',\n",
       "  '4d',\n",
       "  '4e',\n",
       "  '4f',\n",
       "  '4g',\n",
       "  '4k',\n",
       "  '4l',\n",
       "  '4m',\n",
       "  '4n',\n",
       "  '4o',\n",
       "  '4p',\n",
       "  '4r',\n",
       "  '4s',\n",
       "  '4t',\n",
       "  '4u',\n",
       "  '4x',\n",
       "  '4y',\n",
       "  '4z',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '5_',\n",
       "  '5a',\n",
       "  '5b',\n",
       "  '5c',\n",
       "  '5d',\n",
       "  '5e',\n",
       "  '5f',\n",
       "  '5g',\n",
       "  '5h',\n",
       "  '5i',\n",
       "  '5k',\n",
       "  '5m',\n",
       "  '5n',\n",
       "  '5p',\n",
       "  '5r',\n",
       "  '5s',\n",
       "  '5t',\n",
       "  '5v',\n",
       "  '5y',\n",
       "  '5z',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '6_',\n",
       "  '6a',\n",
       "  '6b',\n",
       "  '6d',\n",
       "  '6f',\n",
       "  '6h',\n",
       "  '6i',\n",
       "  '6j',\n",
       "  '6k',\n",
       "  '6l',\n",
       "  '6m',\n",
       "  '6p',\n",
       "  '6r',\n",
       "  '6s',\n",
       "  '6t',\n",
       "  '6v',\n",
       "  '6x',\n",
       "  '6z',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '7_',\n",
       "  '7a',\n",
       "  '7b',\n",
       "  '7c',\n",
       "  '7d',\n",
       "  '7g',\n",
       "  '7i',\n",
       "  '7k',\n",
       "  '7l',\n",
       "  '7m',\n",
       "  '7n',\n",
       "  '7p',\n",
       "  '7r',\n",
       "  '7s',\n",
       "  '7t',\n",
       "  '7v',\n",
       "  '7z',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '8_',\n",
       "  '8a',\n",
       "  '8c',\n",
       "  '8g',\n",
       "  '8h',\n",
       "  '8i',\n",
       "  '8k',\n",
       "  '8m',\n",
       "  '8n',\n",
       "  '8o',\n",
       "  '8r',\n",
       "  '8s',\n",
       "  '8t',\n",
       "  '8v',\n",
       "  '8w',\n",
       "  '8x',\n",
       "  '90',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99',\n",
       "  '9_',\n",
       "  '9a',\n",
       "  '9b',\n",
       "  '9c',\n",
       "  '9d',\n",
       "  '9e',\n",
       "  '9f',\n",
       "  '9g',\n",
       "  '9k',\n",
       "  '9l',\n",
       "  '9m',\n",
       "  '9n',\n",
       "  '9p',\n",
       "  '9s',\n",
       "  '9t',\n",
       "  '9v',\n",
       "  '9x',\n",
       "  '9z',\n",
       "  '_0',\n",
       "  '_1',\n",
       "  '_2',\n",
       "  '_3',\n",
       "  '_4',\n",
       "  '_5',\n",
       "  '_6',\n",
       "  '_7',\n",
       "  '_8',\n",
       "  '_9',\n",
       "  '__',\n",
       "  '_a',\n",
       "  '_b',\n",
       "  '_c',\n",
       "  '_d',\n",
       "  '_e',\n",
       "  '_f',\n",
       "  '_g',\n",
       "  '_h',\n",
       "  '_i',\n",
       "  '_j',\n",
       "  '_k',\n",
       "  '_l',\n",
       "  '_m',\n",
       "  '_n',\n",
       "  '_o',\n",
       "  '_p',\n",
       "  '_q',\n",
       "  '_r',\n",
       "  '_s',\n",
       "  '_t',\n",
       "  '_u',\n",
       "  '_v',\n",
       "  '_w',\n",
       "  '_x',\n",
       "  '_y',\n",
       "  '_z',\n",
       "  'a0',\n",
       "  'a1',\n",
       "  'a2',\n",
       "  'a3',\n",
       "  'a4',\n",
       "  'a5',\n",
       "  'a6',\n",
       "  'a7',\n",
       "  'a8',\n",
       "  'a9',\n",
       "  'a_',\n",
       "  'aa',\n",
       "  'ab',\n",
       "  'ac',\n",
       "  'ad',\n",
       "  'ae',\n",
       "  'af',\n",
       "  'ag',\n",
       "  'ah',\n",
       "  'ai',\n",
       "  'aj',\n",
       "  'ak',\n",
       "  'al',\n",
       "  'am',\n",
       "  'an',\n",
       "  'ao',\n",
       "  'ap',\n",
       "  'aq',\n",
       "  'ar',\n",
       "  'as',\n",
       "  'at',\n",
       "  'au',\n",
       "  'av',\n",
       "  'aw',\n",
       "  'ax',\n",
       "  'ay',\n",
       "  'az',\n",
       "  'b0',\n",
       "  'b1',\n",
       "  'b2',\n",
       "  'b3',\n",
       "  'b4',\n",
       "  'b5',\n",
       "  'b6',\n",
       "  'b9',\n",
       "  'b_',\n",
       "  'ba',\n",
       "  'bb',\n",
       "  'bc',\n",
       "  'bd',\n",
       "  'be',\n",
       "  'bf',\n",
       "  'bg',\n",
       "  'bh',\n",
       "  'bi',\n",
       "  'bj',\n",
       "  'bk',\n",
       "  'bl',\n",
       "  'bm',\n",
       "  'bn',\n",
       "  'bo',\n",
       "  'bp',\n",
       "  'bq',\n",
       "  'br',\n",
       "  'bs',\n",
       "  'bt',\n",
       "  'bu',\n",
       "  'bv',\n",
       "  'bw',\n",
       "  'bx',\n",
       "  'by',\n",
       "  'bz',\n",
       "  'c0',\n",
       "  'c1',\n",
       "  'c2',\n",
       "  'c3',\n",
       "  'c4',\n",
       "  'c6',\n",
       "  'c7',\n",
       "  'c8',\n",
       "  'c9',\n",
       "  'c_',\n",
       "  'ca',\n",
       "  'cb',\n",
       "  'cc',\n",
       "  'cd',\n",
       "  'ce',\n",
       "  'cf',\n",
       "  'cg',\n",
       "  'ch',\n",
       "  'ci',\n",
       "  'cj',\n",
       "  'ck',\n",
       "  'cl',\n",
       "  'cm',\n",
       "  'cn',\n",
       "  'co',\n",
       "  'cp',\n",
       "  'cq',\n",
       "  'cr',\n",
       "  'cs',\n",
       "  'ct',\n",
       "  'cu',\n",
       "  'cv',\n",
       "  'cw',\n",
       "  'cx',\n",
       "  'cy',\n",
       "  'cz',\n",
       "  'd0',\n",
       "  'd1',\n",
       "  'd2',\n",
       "  'd3',\n",
       "  'd4',\n",
       "  'd5',\n",
       "  'd6',\n",
       "  'd7',\n",
       "  'd8',\n",
       "  'd9',\n",
       "  'd_',\n",
       "  'da',\n",
       "  'db',\n",
       "  'dc',\n",
       "  'dd',\n",
       "  'de',\n",
       "  'df',\n",
       "  'dg',\n",
       "  'dh',\n",
       "  'di',\n",
       "  'dj',\n",
       "  'dk',\n",
       "  'dl',\n",
       "  'dm',\n",
       "  'dn',\n",
       "  'do',\n",
       "  'dp',\n",
       "  'dq',\n",
       "  'dr',\n",
       "  'ds',\n",
       "  'dt',\n",
       "  'du',\n",
       "  'dv',\n",
       "  'dw',\n",
       "  'dx',\n",
       "  'dy',\n",
       "  'dz',\n",
       "  'e0',\n",
       "  'e1',\n",
       "  'e2',\n",
       "  'e3',\n",
       "  'e4',\n",
       "  'e5',\n",
       "  'e6',\n",
       "  'e7',\n",
       "  'e8',\n",
       "  'e9',\n",
       "  'e_',\n",
       "  'ea',\n",
       "  'eb',\n",
       "  'ec',\n",
       "  'ed',\n",
       "  'ee',\n",
       "  'ef',\n",
       "  'eg',\n",
       "  'eh',\n",
       "  'ei',\n",
       "  'ej',\n",
       "  'ek',\n",
       "  'el',\n",
       "  'em',\n",
       "  'en',\n",
       "  'eo',\n",
       "  'ep',\n",
       "  'eq',\n",
       "  'er',\n",
       "  'es',\n",
       "  'et',\n",
       "  'eu',\n",
       "  'ev',\n",
       "  'ew',\n",
       "  'ex',\n",
       "  'ey',\n",
       "  'ez',\n",
       "  'f0',\n",
       "  'f1',\n",
       "  'f2',\n",
       "  'f3',\n",
       "  'f4',\n",
       "  'f6',\n",
       "  'f7',\n",
       "  'f9',\n",
       "  'f_',\n",
       "  'fa',\n",
       "  'fb',\n",
       "  'fc',\n",
       "  'fd',\n",
       "  'fe',\n",
       "  'ff',\n",
       "  'fg',\n",
       "  'fh',\n",
       "  'fi',\n",
       "  'fj',\n",
       "  'fk',\n",
       "  'fl',\n",
       "  'fm',\n",
       "  'fn',\n",
       "  'fo',\n",
       "  'fp',\n",
       "  'fq',\n",
       "  'fr',\n",
       "  'fs',\n",
       "  'ft',\n",
       "  'fu',\n",
       "  'fv',\n",
       "  'fw',\n",
       "  'fx',\n",
       "  'fy',\n",
       "  'fz',\n",
       "  'g0',\n",
       "  'g1',\n",
       "  'g2',\n",
       "  'g3',\n",
       "  'g4',\n",
       "  'g5',\n",
       "  'g6',\n",
       "  'g7',\n",
       "  'g8',\n",
       "  'g9',\n",
       "  'g_',\n",
       "  'ga',\n",
       "  'gb',\n",
       "  'gc',\n",
       "  'gd',\n",
       "  'ge',\n",
       "  'gf',\n",
       "  'gg',\n",
       "  'gh',\n",
       "  'gi',\n",
       "  'gj',\n",
       "  'gk',\n",
       "  'gl',\n",
       "  'gm',\n",
       "  'gn',\n",
       "  'go',\n",
       "  'gp',\n",
       "  'gq',\n",
       "  'gr',\n",
       "  'gs',\n",
       "  'gt',\n",
       "  'gu',\n",
       "  'gv',\n",
       "  'gw',\n",
       "  'gx',\n",
       "  'gy',\n",
       "  'gz',\n",
       "  'h0',\n",
       "  'h1',\n",
       "  'h2',\n",
       "  'h3',\n",
       "  'h4',\n",
       "  'h5',\n",
       "  'h6',\n",
       "  'h7',\n",
       "  'h8',\n",
       "  'h9',\n",
       "  'h_',\n",
       "  'ha',\n",
       "  'hb',\n",
       "  'hc',\n",
       "  'hd',\n",
       "  'he',\n",
       "  'hf',\n",
       "  'hg',\n",
       "  'hh',\n",
       "  'hi',\n",
       "  'hj',\n",
       "  'hk',\n",
       "  'hl',\n",
       "  'hm',\n",
       "  'hn',\n",
       "  'ho',\n",
       "  'hp',\n",
       "  'hq',\n",
       "  'hr',\n",
       "  'hs',\n",
       "  'ht',\n",
       "  'hu',\n",
       "  'hv',\n",
       "  'hw',\n",
       "  'hx',\n",
       "  'hy',\n",
       "  'hz',\n",
       "  'i0',\n",
       "  'i1',\n",
       "  'i2',\n",
       "  'i3',\n",
       "  'i4',\n",
       "  'i5',\n",
       "  'i6',\n",
       "  'i7',\n",
       "  'i8',\n",
       "  'i9',\n",
       "  'i_',\n",
       "  'ia',\n",
       "  'ib',\n",
       "  'ic',\n",
       "  'id',\n",
       "  'ie',\n",
       "  'if',\n",
       "  'ig',\n",
       "  'ih',\n",
       "  'ii',\n",
       "  'ij',\n",
       "  'ik',\n",
       "  'il',\n",
       "  'im',\n",
       "  'in',\n",
       "  'io',\n",
       "  'ip',\n",
       "  'iq',\n",
       "  'ir',\n",
       "  'is',\n",
       "  'it',\n",
       "  'iu',\n",
       "  'iv',\n",
       "  'iw',\n",
       "  'ix',\n",
       "  'iy',\n",
       "  'iz',\n",
       "  'j0',\n",
       "  'j1',\n",
       "  'j2',\n",
       "  'j3',\n",
       "  'j4',\n",
       "  'j5',\n",
       "  'j6',\n",
       "  'j7',\n",
       "  'j9',\n",
       "  'j_',\n",
       "  'ja',\n",
       "  'jb',\n",
       "  'jc',\n",
       "  'jd',\n",
       "  'je',\n",
       "  'jf',\n",
       "  'jg',\n",
       "  'jh',\n",
       "  'ji',\n",
       "  'jj',\n",
       "  'jk',\n",
       "  'jl',\n",
       "  'jm',\n",
       "  'jn',\n",
       "  'jo',\n",
       "  'jp',\n",
       "  'jq',\n",
       "  'jr',\n",
       "  'js',\n",
       "  'jt',\n",
       "  'ju',\n",
       "  'jv',\n",
       "  'jw',\n",
       "  'jx',\n",
       "  'jy',\n",
       "  'jz',\n",
       "  'k0',\n",
       "  'k1',\n",
       "  'k2',\n",
       "  'k3',\n",
       "  'k4',\n",
       "  'k5',\n",
       "  'k6',\n",
       "  'k7',\n",
       "  'k8',\n",
       "  'k9',\n",
       "  'k_',\n",
       "  'ka',\n",
       "  'kb',\n",
       "  'kc',\n",
       "  'kd',\n",
       "  'ke',\n",
       "  'kf',\n",
       "  'kg',\n",
       "  'kh',\n",
       "  'ki',\n",
       "  'kj',\n",
       "  'kk',\n",
       "  'kl',\n",
       "  'km',\n",
       "  'kn',\n",
       "  'ko',\n",
       "  'kp',\n",
       "  'kq',\n",
       "  'kr',\n",
       "  'ks',\n",
       "  'kt',\n",
       "  'ku',\n",
       "  'kv',\n",
       "  'kw',\n",
       "  'kx',\n",
       "  'ky',\n",
       "  'kz',\n",
       "  'l0',\n",
       "  'l1',\n",
       "  'l2',\n",
       "  'l3',\n",
       "  'l4',\n",
       "  'l5',\n",
       "  'l6',\n",
       "  'l7',\n",
       "  'l8',\n",
       "  'l9',\n",
       "  'l_',\n",
       "  'la',\n",
       "  'lb',\n",
       "  'lc',\n",
       "  'ld',\n",
       "  'le',\n",
       "  'lf',\n",
       "  'lg',\n",
       "  'lh',\n",
       "  'li',\n",
       "  'lj',\n",
       "  'lk',\n",
       "  'll',\n",
       "  'lm',\n",
       "  'ln',\n",
       "  'lo',\n",
       "  'lp',\n",
       "  'lq',\n",
       "  'lr',\n",
       "  'ls',\n",
       "  'lt',\n",
       "  'lu',\n",
       "  'lv',\n",
       "  'lw',\n",
       "  'lx',\n",
       "  'ly',\n",
       "  'lz',\n",
       "  'm0',\n",
       "  'm1',\n",
       "  'm2',\n",
       "  'm3',\n",
       "  'm4',\n",
       "  'm5',\n",
       "  'm6',\n",
       "  'm7',\n",
       "  'm8',\n",
       "  'm9',\n",
       "  'm_',\n",
       "  'ma',\n",
       "  'mb',\n",
       "  'mc',\n",
       "  'md',\n",
       "  'me',\n",
       "  'mf',\n",
       "  'mg',\n",
       "  'mh',\n",
       "  'mi',\n",
       "  'mj',\n",
       "  'mk',\n",
       "  'ml',\n",
       "  'mm',\n",
       "  'mn',\n",
       "  'mo',\n",
       "  'mp',\n",
       "  'mq',\n",
       "  'mr',\n",
       "  'ms',\n",
       "  'mt',\n",
       "  'mu',\n",
       "  'mv',\n",
       "  'mw',\n",
       "  'mx',\n",
       "  'my',\n",
       "  'mz',\n",
       "  'n0',\n",
       "  'n1',\n",
       "  'n2',\n",
       "  'n3',\n",
       "  'n4',\n",
       "  'n5',\n",
       "  'n6',\n",
       "  'n7',\n",
       "  'n8',\n",
       "  'n9',\n",
       "  'n_',\n",
       "  'na',\n",
       "  'nb',\n",
       "  'nc',\n",
       "  'nd',\n",
       "  'ne',\n",
       "  'nf',\n",
       "  'ng',\n",
       "  'nh',\n",
       "  'ni',\n",
       "  'nj',\n",
       "  'nk',\n",
       "  'nl',\n",
       "  'nm',\n",
       "  'nn',\n",
       "  'no',\n",
       "  'np',\n",
       "  'nq',\n",
       "  'nr',\n",
       "  'ns',\n",
       "  'nt',\n",
       "  'nu',\n",
       "  'nv',\n",
       "  'nw',\n",
       "  'nx',\n",
       "  'ny',\n",
       "  'nz',\n",
       "  'o0',\n",
       "  'o1',\n",
       "  'o2',\n",
       "  'o3',\n",
       "  'o4',\n",
       "  'o5',\n",
       "  'o6',\n",
       "  'o7',\n",
       "  'o8',\n",
       "  'o9',\n",
       "  'o_',\n",
       "  'oa',\n",
       "  'ob',\n",
       "  'oc',\n",
       "  'od',\n",
       "  'oe',\n",
       "  'of',\n",
       "  'og',\n",
       "  'oh',\n",
       "  'oi',\n",
       "  'oj',\n",
       "  'ok',\n",
       "  'ol',\n",
       "  'om',\n",
       "  'on',\n",
       "  'oo',\n",
       "  'op',\n",
       "  'oq',\n",
       "  'or',\n",
       "  'os',\n",
       "  'ot',\n",
       "  'ou',\n",
       "  'ov',\n",
       "  'ow',\n",
       "  'ox',\n",
       "  'oy',\n",
       "  'oz',\n",
       "  'p0',\n",
       "  'p1',\n",
       "  'p2',\n",
       "  'p3',\n",
       "  'p5',\n",
       "  'p6',\n",
       "  'p7',\n",
       "  'p8',\n",
       "  'p9',\n",
       "  'p_',\n",
       "  'pa',\n",
       "  'pb',\n",
       "  'pc',\n",
       "  'pd',\n",
       "  'pe',\n",
       "  'pf',\n",
       "  'pg',\n",
       "  'ph',\n",
       "  'pi',\n",
       "  'pj',\n",
       "  'pk',\n",
       "  'pl',\n",
       "  'pm',\n",
       "  'pn',\n",
       "  'po',\n",
       "  'pp',\n",
       "  'pq',\n",
       "  'pr',\n",
       "  'ps',\n",
       "  'pt',\n",
       "  'pu',\n",
       "  'pv',\n",
       "  'pw',\n",
       "  'px',\n",
       "  'py',\n",
       "  'pz',\n",
       "  'q1',\n",
       "  'q2',\n",
       "  'q3',\n",
       "  'q8',\n",
       "  'q9',\n",
       "  'q_',\n",
       "  'qa',\n",
       "  'qb',\n",
       "  'qc',\n",
       "  'qd',\n",
       "  'qe',\n",
       "  'qg',\n",
       "  'qh',\n",
       "  'qi',\n",
       "  'qk',\n",
       "  'ql',\n",
       "  'qm',\n",
       "  'qn',\n",
       "  'qo',\n",
       "  'qq',\n",
       "  'qr',\n",
       "  'qs',\n",
       "  'qt',\n",
       "  'qu',\n",
       "  'qv',\n",
       "  'qw',\n",
       "  'qx',\n",
       "  'qy',\n",
       "  'qz',\n",
       "  'r0',\n",
       "  'r1',\n",
       "  'r2',\n",
       "  'r3',\n",
       "  'r4',\n",
       "  'r5',\n",
       "  'r6',\n",
       "  'r7',\n",
       "  'r8',\n",
       "  'r9',\n",
       "  'r_',\n",
       "  'ra',\n",
       "  'rb',\n",
       "  'rc',\n",
       "  'rd',\n",
       "  're',\n",
       "  'rf',\n",
       "  'rg',\n",
       "  'rh',\n",
       "  'ri',\n",
       "  'rj',\n",
       "  'rk',\n",
       "  'rl',\n",
       "  'rm',\n",
       "  'rn',\n",
       "  'ro',\n",
       "  'rp',\n",
       "  'rq',\n",
       "  'rr',\n",
       "  'rs',\n",
       "  'rt',\n",
       "  'ru',\n",
       "  'rv',\n",
       "  'rw',\n",
       "  'rx',\n",
       "  'ry',\n",
       "  'rz',\n",
       "  's0',\n",
       "  's1',\n",
       "  's2',\n",
       "  's3',\n",
       "  's4',\n",
       "  's5',\n",
       "  's6',\n",
       "  's7',\n",
       "  's8',\n",
       "  's9',\n",
       "  's_',\n",
       "  'sa',\n",
       "  'sb',\n",
       "  'sc',\n",
       "  'sd',\n",
       "  'se',\n",
       "  'sf',\n",
       "  'sg',\n",
       "  'sh',\n",
       "  ...])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "vectorized_screenName_df = vectorizer.fit_transform(tweets_df['screenName'])\n",
    "\n",
    "vectorized_screenName_df.shape, vectorizer.get_feature_names(), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIMEð¨ð»âð¬\n",
    "##### Experiment 7_4\n",
    "\n",
    "This experiment uses 2-gram bag of chars for screenName feature and logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_7_4: File exists\n",
      "mkdir: experiments/exp_7_4/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_4\n",
    "!mkdir experiments/exp_7_4/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_4/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/config.yaml\n",
    "\n",
    "description: This experiment uses 2-gram bag of chars for screenName feature and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: characters paired 2 by 2\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l1\n",
    "logistic_rand_state: 2020\n",
    "pca_components: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_4/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_4(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        nltk.download('tagsets')\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
    "        transformer_df = vectorizer.fit_transform(X['screenName'])\n",
    "        return transformer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_4/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_4/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_4.transformer import Transformer_7_4\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = 'screenName'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_4/config.yaml', Transformer_7_4, 'experiments/exp_7_4/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_4/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 454, 'accuracy': 0.5016666666666667, 'precision': 0.5098305084745762, 'recall': 0.49343832020997375, 'mutual_info_score': -0.00023131604942293197}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_4/result/result_02-Nov-2020-21-30-01.yaml\n",
      "CPU times: user 21.7 s, sys: 520 ms, total: 22.3 s\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_4/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polynomial features for presence of avenger character names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%store -r tweets_df\n",
    "tweets_txt_df = tweets_df[['text']]\n",
    "\n",
    "heroes = [\"Iron Man\", \"Thor\", \"Captain America\", \"Captain Marvel\", \"Black Widow\", \"Hawkeye\", \"Hulk\", \"Vision\", \"Scarlet Witch\", \"War Machine\", \"Falcon\", \"StarLord\", \"Rocket Raccoon\", \"Groot\", \"Gamora\", \"Drax\", \"Mantis\", \"Nebula\", \"Doctor Strange\", \"Wong\", \"Spider-Man\", \"Spiderman\", \"Winter Soldier\", \"Heimdall\", \"Black Panther\", \"Okoye\", \"Shuri\", \"MâBaku\", \"Eitiri\", \"Nick Fury\", \"Maria Hill\", \"Pepper Potts\", \"William âThunderboltâ Ross\", \"Ned\", \"Thanos\", \"Loki\", \"the Collector\", \"Cull Obsidian\", \"Ebony Maw\", \"Proxima Midnight\", \"Corvus Glaive\", \"Red Skull\", \"The Wasp\"]\n",
    "heroes = [h.lower().replace(\" \", \"\") for h in heroes]\n",
    "\n",
    "cv = CountVectorizer(vocabulary=heroes)\n",
    "\n",
    "heroes_count_df = pd.DataFrame.sparse.from_spmatrix(cv.fit_transform(tweets_txt_df['text']), \n",
    "                   tweets_txt_df.index,\n",
    "                   cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_count_arr = heroes_count_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 15180)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "transformed_df = poly.fit_transform(heroes_count_arr)\n",
    "\n",
    "transformed_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT TIMEð¨ð»âð¬\n",
    "##### Experiment 7_5\n",
    "\n",
    "This experiment uses polynomial features for presence of hero name in text feature and logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: experiments: File exists\n",
      "mkdir: experiments/exp_7_5: File exists\n",
      "mkdir: experiments/exp_7_5/result: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir experiments\n",
    "!mkdir experiments/exp_7_5\n",
    "!mkdir experiments/exp_7_5/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_5/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/config.yaml\n",
    "\n",
    "description: This experiment uses polynomial features for presence of hero name in text feature and logistic regression for classification.\n",
    "    Choosing [None] for pca_components will automate the process by selecting the components with sum variance > .9\n",
    "    \n",
    "features: polynomial features of dummy variables\n",
    "target: High_Retweet_Count\n",
    "    \n",
    "config_variables:\n",
    "    - logistic_penalty\n",
    "    - logistic_rand_state\n",
    "    - pca_components\n",
    "    - target\n",
    "    \n",
    "logistic_penalty: l2\n",
    "logistic_rand_state: 2020\n",
    "pca_components: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_5/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/transformer.py\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from experiments.base.transformer import BaseTransformer\n",
    "\n",
    "\n",
    "class Transformer_7_5(BaseTransformer):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        X = X[['text']]\n",
    "\n",
    "        heroes = [\"Iron Man\", \"Thor\", \"Captain America\", \"Captain Marvel\", \"Black Widow\", \"Hawkeye\", \"Hulk\", \"Vision\", \"Scarlet Witch\", \"War Machine\", \"Falcon\", \"StarLord\", \"Rocket Raccoon\", \"Groot\", \"Gamora\", \"Drax\", \"Mantis\", \"Nebula\", \"Doctor Strange\", \"Wong\", \"Spider-Man\", \"Spiderman\", \"Winter Soldier\", \"Heimdall\", \"Black Panther\", \"Okoye\", \"Shuri\", \"MâBaku\", \"Eitiri\", \"Nick Fury\", \"Maria Hill\", \"Pepper Potts\", \"William âThunderboltâ Ross\", \"Ned\", \"Thanos\", \"Loki\", \"the Collector\", \"Cull Obsidian\", \"Ebony Maw\", \"Proxima Midnight\", \"Corvus Glaive\", \"Red Skull\", \"The Wasp\"]\n",
    "        heroes = [h.lower().replace(\" \", \"\") for h in heroes]\n",
    "\n",
    "        cv = CountVectorizer(vocabulary=heroes)\n",
    "\n",
    "        heroes_count_df = pd.DataFrame.sparse.from_spmatrix(cv.fit_transform(X['text']), \n",
    "                           X.index,\n",
    "                           cv.get_feature_names())\n",
    "        \n",
    "        heroes_count_arr = heroes_count_df.to_numpy()\n",
    "        \n",
    "        poly = PolynomialFeatures(degree=3)\n",
    "        transformed_df = poly.fit_transform(heroes_count_arr)\n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/exp_7_5/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/exp_7_5/main.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from experiments.exp_7_5.transformer import Transformer_7_5\n",
    "from experiments.base.classifier import LogisticCustomClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "features = 'text'\n",
    "target = 'High_Retweet_Count'\n",
    "\n",
    "classifier = LogisticCustomClassifier('experiments/exp_7_5/config.yaml', Transformer_7_5, 'experiments/exp_7_5/result/')\n",
    "classifier.fit(X_train, y_train[target])\n",
    "y_pred = classifier.predict(X_test)\n",
    "classifier.generate_results(y_pred, y_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration file experiments/exp_7_5/config.yaml:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python37/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "{'pca_components': 10, 'accuracy': 0.38533333333333336, 'precision': 0.7437288135593221, 'recall': 0.4280140460397971, 'mutual_info_score': 0.09368617357974225}\n",
      "------------------\n",
      "Results are saved to: experiments/exp_7_5/result/result_02-Nov-2020-21-49-42.yaml\n",
      "CPU times: user 1h 7min 32s, sys: 38.5 s, total: 1h 8min 10s\n",
      "Wall time: 17min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run experiments/exp_7_5/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8: Apply PCA dimensionality reduction and LogisticRegression to predict Target, construct pipelines for all transformers from Exercise 5 and 7 (name them exp_[your name acronim]\\_6_1, ... exp_[your name acronim]\\_6_...) implement them as custom classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I did experiements 5_1, 7_1, 7_2, 7_3, 7_4, 7_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: Split dataset to train and test (it is up to you which features you will include in it) and store it data folder along with .yaml description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>...</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>High_Retweet_Count</th>\n",
       "      <th>number_of_words</th>\n",
       "      <th>number_of_chars</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-23 10:43:30</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120639328034676737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>DavidAc96</td>\n",
       "      <td>637</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @mrvelstan: literally nobody:\\r\\nme:\\r\\n\\r\\...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2019-04-23 10:43:30      False         NaN   \n",
       "\n",
       "                    id  replyToUID  ... screenName retweetCount  isRetweet  \\\n",
       "0  1120639328034676737         NaN  ...  DavidAc96          637       True   \n",
       "\n",
       "   retweeted  longitude  latitude  High_Retweet_Count  number_of_words  \\\n",
       "0      False        NaN       NaN                   0                7   \n",
       "\n",
       "   number_of_chars  number_of_hashtags  \n",
       "0               81                   1  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r tweets_df\n",
    "tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./data/split: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir ./data/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = ['text', 'favoriteCount', 'screenName']\n",
    "target = ['High_Retweet_Count']\n",
    "\n",
    "tweets_df['text'] = tweets_df['text'].apply(clean_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df[features], \\\n",
    "                                                    tweets_df[target], \\\n",
    "                                                    test_size=0.2, \\\n",
    "                                                    random_state=2020)\n",
    "\n",
    "pickle.dump(train_test_split(tweets_df[features], \\\n",
    "                                tweets_df[target], \\\n",
    "                                test_size=0.2, \\\n",
    "                                random_state=2020), open('./data/split/split.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/split/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/split/config.yaml\n",
    "\n",
    "Description: Split for ./data/new_tweets/new_tweets.csv\n",
    "    \n",
    "test_size: 0.2\n",
    "random_state: 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13698    BrandonDavisBD AvengersEndgame astonishing ama...\n",
       "1142     Marvel Black Widow Scarlett Johansson BlackWid...\n",
       "7954     caplovesfondue Chris Evans brought WHOLE FAMIL...\n",
       "2225     targrycn honestly chris evans saying cried six...\n",
       "9350     Marvel Black Widow Scarlett Johansson BlackWid...\n",
       "                               ...                        \n",
       "11971    Us trying book AvengersEndgame weekend hammad1998\n",
       "14966         kookiewoman part journey end AvengersEndgame\n",
       "7491     PNemiroff AvengersEndgame A LOT Too much times...\n",
       "12680    Factory Farmed Salmon Full Disease Hazardous C...\n",
       "9056     hmvtweets WIN AvengersEndgame merch bundle Sim...\n",
       "Name: text, Length: 12000, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pickle.load(open('./data/split/split.pickle', 'rb'))\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: Add intialization from .yaml descriptions of classifiers to implementations of classifiers at Exercise 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 11: Train classifiers with various PCA dimensionality, bag of words and polynomial paramters paramteres on train, test them on test and store in .yaml files for every experiment with resulting metrics\n",
    "\n",
    "- accuracy\n",
    "- precision\n",
    "- recall\n",
    "- adjusted_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 12*: Read urls on dataset follow them with the following example, download attached images and count people on them with cv features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13*: Add new features to all PCA output on Exercise 6, train classifiers, test quality and store results in proper .yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14*: Perform topic modelings on text feature, add it to PCA features from Exercise 8, train classifiers, test quality and store results in proper .yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Combine all combinations of features from Exercise 8, measure the results of improvement of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16*: For 5 best of prevous experiments, change LogisticRegression to XGBClassifier. Try several XGB configurations, store results and parameters in .yaml files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 17: in collaboration with classmates create the best possible classifier for the data using various technics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
